# Посібник: Вступ до Scrapy

Ласкаво просимо до посібника по Scrapy! Якщо Beautiful Soup — це скальпель для точного видобування даних з однієї сторінки, то **Scrapy — це ціла фабрика** для автоматизованого збору даних у великих масштабах.

**Цей посібник об'єднує теорію та практику.** Спочатку ми розберемося в архітектурі фреймворку, а потім крок за кроком запустимо нашого першого "павука".

---

## Частина 1: Теоретичні основи Scrapy

### 1.1. Що таке Scrapy і коли його використовувати?

**Scrapy** — це потужний Python-фреймворк для веб-скрапінгу та веб-кроулінгу (автоматичного переходу по посиланнях).

| Використовуйте Scrapy, якщо:                     | Використовуйте Beautiful Soup, якщо:        |
| ------------------------------------------------ | ------------------------------------------- |
| Вам потрібно зібрати дані з тисяч сторінок.      | Вам потрібно витягти дані з кількох сторінок. |
| Потрібна висока швидкість (асинхронні запити).   | Достатньо простого послідовного скрипта.    |
| Потрібен комплексний процес (очищення, валідація). | Потрібен лише парсинг HTML.                 |
| Ви створюєте довготривалий проєкт для збору даних. | Ви пишете невеликий одноразовий скрипт.     |

### 1.2. Архітектура Scrapy

Scrapy має чітку архітектуру, де кожен компонент виконує свою роль:

```
┌───────────────────┐      ┌──────────────┐      ┌───────────────────┐
│      SPIDER       │───►│    ENGINE    │◄───│    SCHEDULER    │
│ (Ваш код парсингу)│◄───│ (Координатор)│───►│ (Черга посилань)  │
└───────────────────┘      └──────┬───────┘      └───────────────────┘
                                  │
                                  ▼
┌───────────────────┐      ┌──────────────┐
│ ITEM PIPELINES    │◄───│  DOWNLOADER  │
│(Обробка даних)    │      │(Завантажувач)│
└───────────────────┘      └──────────────┘
```

- **Engine (Двигун):** Центральний компонент, який керує всім процесом.
- **Scheduler (Планувальник):** Приймає від павука посилання, ставить їх у чергу і видає Двигуну.
- **Downloader (Завантажувач):** Завантажує веб-сторінки за посиланнями.
- **Spider (Павук):** **Це ваш основний код.** Він визначає, як парсити сторінку, які дані видобувати та куди переходити далі.
- **Item Pipelines (Конвеєри):** Обробляють зібрані дані: валідують, очищують, видаляють дублікати та зберігають у базу даних або файл.

### 1.3. Ключові файли проєкту

- `settings.py`: Глобальні налаштування проєкту (швидкість, паралельність, які конвеєри використовувати).
- `items.py`: Визначення структури даних, які ви збираєте (наприклад, `QuoteItem` з полями `text`, `author`).
- `spiders/`: Папка, де живуть ваші павуки.
- `pipelines.py`: Код для обробки зібраних даних.

---

## Частина 2: Практичний запуск павука

Тепер давайте запустимо готового павука, який збирає цитати.

### 2.1. Підготовка
Перейдіть до папки з проєктом Scrapy:
```bash
cd /root/goit/python_web/module_10/02_advanced_edition/code/scrapy_project
```

### 2.2. Запуск павука
Виконайте в терміналі:
```bash
scrapy crawl quotes
```
- `scrapy crawl`: команда для запуску.
- `quotes`: ім'я павука, яке вказано в файлі `spiders/quotes_spider.py` (`name = 'quotes'`).

Ви побачите в консолі логи, які показують, як Scrapy завантажує сторінки та знаходить дані.

### 2.3. Збереження результатів у файл
Щоб зберегти зібрані дані, додайте прапор `-o` (output):
```bash
scrapy crawl quotes -o quotes.json
```
Ця команда створить файл `quotes.json` з усіма зібраними цитатами.

### 2.4. Аналіз коду павука (`spiders/quotes_spider.py`)

Давайте розберемо ключові частини:
```python
class QuotesSpider(scrapy.Spider):
    # Ім'я павука, яке ми використовуємо в команді 'scrapy crawl'
    name = 'quotes'
    
    # Домени, за межі яких павук не вийде
    allowed_domains = ['quotes.toscrape.com']
    
    # URL, з яких павук починає свою роботу
    start_urls = ['https://quotes.toscrape.com/']

    def parse(self, response):
        """
        Основний метод, який обробляє завантажену сторінку.
        'response' — це об'єкт, що містить HTML-код сторінки.
        """
        # Шукаємо всі <div> з класом 'quote'
        for quote_div in response.css('div.quote'):
            # Видобуваємо дані за допомогою CSS-селекторів
            yield {
                'text': quote_div.css('span.text::text').get(),
                'author': quote_div.css('small.author::text').get(),
                'tags': quote_div.css('a.tag::text').getall(),
            }

        # Знаходимо посилання на наступну сторінку
        next_page = response.css('li.next a::attr(href)').get()
        if next_page:
            # Створюємо новий запит на наступну сторінку і вказуємо,
            # що її також потрібно обробити цим же методом 'parse'
            yield response.follow(next_page, callback=self.parse)
```
- **`yield`**: ключове слово в Scrapy. Коли ви робите `yield` словника, Scrapy розуміє, що це зібраний `item`, і відправляє його в Item Pipeline. Коли ви робите `yield` запиту (`response.follow`), Scrapy додає посилання в чергу для подальшого завантаження.

### 2.5. Інтерактивна консоль `Scrapy Shell`
Це неймовірно зручний інструмент для тестування селекторів в реальному часі.
```bash
# Запускаємо shell для конкретної сторінки
scrapy shell 'https://quotes.toscrape.com'
```
Тепер ви в інтерактивній консолі, де доступний об'єкт `response`. Можна тестувати селектори:
```python
# В консолі Scrapy Shell
>>> response.css('title::text').get()
'Quotes to Scrape'

>>> response.css('div.quote span.text::text').get()
'"The world as we have created it is a process of our thinking..."'

>>> response.css('li.next a::attr(href)').get()
'/page/2/'
```
Це дозволяє підібрати правильні селектори перед тим, як писати код павука.

---

## Частина 3: Найкращі практики та налаштування

### 3.1. "Ввічливий" скрапінг
Щоб не створювати проблем сайту, який ви скрапите, та не отримати бан, дотримуйтесь правил:
- **Затримки між запитами:** В `settings.py` встановіть затримку.
  ```python
  DOWNLOAD_DELAY = 1  # Затримка в 1 секунду
  ```
- **Поважайте `robots.txt`:** Це файл, де власники сайтів вказують, що можна, а що не можна скрапити. Scrapy за замовчуванням його поважає.
  ```python
  ROBOTSTXT_OBEY = True
  ```
- **Представляйтеся:** Встановіть `USER_AGENT`, щоб сайт знав, хто робить запити.
  ```python
  USER_AGENT = 'MyAwesomeScraper (+http://www.mywebsite.com)'
  ```

### 3.2. Обробка даних в `pipelines.py`
Кожен `item`, який ви збираєте, проходить через конвеєр. Це ідеальне місце для:
- **Валідації:** перевірки, чи всі потрібні поля заповнені.
- **Очищення:** видалення зайвих пробілів, HTML-тегів.
- **Видалення дублікатів.**
- **Збереження в базу даних.**

**Приклад простого конвеєра:**
```python
# pipelines.py
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

class ValidationPipeline:
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        if not adapter.get('text') or not adapter.get('author'):
            raise DropItem(f"Missing data in {item}")
        return item
```
Щоб активувати цей конвеєр, його потрібно додати в `settings.py`.

---

## Висновок

Ви ознайомилися з основами Scrapy та запустили свого першого павука. Scrapy — це потужний інструмент, і ми лише подряпали поверхню.

**Ключові ідеї:**
- Scrapy — це фреймворк для великих задач.
- Павуки (`Spiders`) — це ваш основний код для парсингу.
- CSS-селектори та XPath — інструменти для пошуку даних.
- `yield` використовується для повернення даних та нових посилань.
- `Scrapy Shell` — ваш найкращий друг для розробки та тестування.
- `Pipelines` — для обробки та збереження даних.

## Корисні посилання
- **[Офіційний туторіал Scrapy](https://docs.scrapy.org/en/latest/intro/tutorial.html)**
- **[Архітектура Scrapy](https://docs.scrapy.org/en/latest/topics/architecture.html)**
- **[CSS Selectors Reference](https://www.w3schools.com/cssref/css_selectors.php)**
