# Урок 1: Концепції фреймворку Scrapy

**Мета**: Зрозуміти архітектуру Scrapy та коли його використовувати.
**Передумови**: Основи BeautifulSoup з початкового рівня.

---

## Що таке Scrapy?

Scrapy — це **фреймворк на Python для великомасштабного веб-скрапінгу та веб-кроулінгу**. На відміну від BeautifulSoup (який є лише парсером HTML), Scrapy — це повноцінний фреймворк, який обробляє:

- **Керування запитами** - одночасні запити, повторні спроби, обмеження швидкості.
- **Парсинг HTML** - за допомогою CSS/XPath селекторів.
- **Видобування даних** - структуровані конвеєри даних.
- **Збереження** - у бази даних, файли, API.
- **Планування** - періодичний кроулінг.
- **Моніторинг** - статистика запитів/відповідей.

### Коли використовувати Scrapy

| Використовуйте Scrapy | Використовуйте BeautifulSoup |
|-----------|------------------|
| 100,000+ сторінок для скрапінгу | < 1,000 сторінок |
| Потрібна паралельність | Достатньо однопотоковості |
| Складні конвеєри | Простий парсинг HTML |
| Тривалі кроулінги | Одноразові скрипти |
| Продакшн системи | Навчання/експерименти |

---

## Огляд архітектури

Scrapy слідує **патерну конвеєра**, де дані проходять через компоненти:

```
┌─────────────────────────────────────────────────────────┐
│                    SCRAPY ENGINE                        │
│  (Координатор: керує всіма іншими компонентами)         │
└──────────────┬──────────────────┬──────────────────────┘
               │                  │
               ↓                  ↓
      ┌────────────────┐  ┌──────────────────┐
      │  SCHEDULER     │  │  DOWNLOADER      │
      │                │  │                  │
      │ Керує чергою   │  │ Завантажує сторінки│
      │ URL-адрес      │  │ через HTTP       │
      └────────┬───────┘  └────────┬─────────┘
               │                   │
               ↓                   ↓
      ┌────────────────────────────────────┐
      │         SPIDER                      │
      │                                    │
      │ Парсить HTML, видобуває дані       │
      │ Повертає елементи та нові URL-адреси│
      └────────────────┬───────────────────┘
                       │
                       ↓
      ┌────────────────────────────────────┐
      │      ITEM PIPELINES                │
      │                                    │
      │ 1. ValidationPipeline              │
      │ 2. DuplicateFilterPipeline         │
      │ 3. DatabasePipeline                │
      │ 4. JsonExportPipeline              │
      └────────────────────────────────────┘
```

### Ключові компоненти

- **Engine**: Центральний координатор, який керує потоком даних.
- **Scheduler**: Підтримує чергу URL-адрес для кроулінгу.
- **Downloader**: Завантажує веб-сторінки.
- **Spider**: Ваш власний код, який парсить HTML та видобуває дані.
- **Item Pipelines**: Обробляють видобуті дані (валідація, очищення, збереження).

---

## Видобування даних

Scrapy використовує **CSS-селектори** та **XPath** для видобування:

### CSS-селектори (простіше)

```python
response.css('div.quote')        # <div class="quote">
response.css('a::attr(href)')    # атрибут href
response.css('h1::text')         # текстовий вміст
```

### XPath (потужніше)

```python
response.xpath('//div[@class="quote"]')
response.xpath('//h1/text()')
```

---
## Додаткові ресурси

- **Туторіал Scrapy**: https://docs.scrapy.org/en/latest/intro/tutorial.html
- **Архітектура Scrapy**: https://docs.scrapy.org/en/latest/topics/architecture.html
