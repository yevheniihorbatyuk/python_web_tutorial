# Урок 4: Патерни для продакшну для масштабованості та надійності

**Мета**: вивчити патерни, які роблять системи скрапінгу готовими до продакшну.

---

## Що робить систему "готовою до продакшну"?

Продакшн-системи вимагають більше, ніж просто робочий код. Вони повинні бути:
- **Надійними**: витончено обробляти помилки та автоматично відновлюватися.
- **Масштабованими**: обробляти великий обсяг завдань і даних.
- **Спостережуваними**: надавати уявлення про те, що робить система (логування та моніторинг).
- **Ефективними**: розумно використовувати ресурси, такі як процесор, пам'ять та мережа.

---

## Патерн 1: Кешування за допомогою Redis

**Проблема**: повторне отримання тих самих даних з бази даних є повільним.
**Рішення**: **кешування**. Зберігайте часто використовувані дані у швидкому сховищі в пам'яті, такому як Redis.

### Патерн Cache-Aside

1.  Додаток запитує дані.
2.  Спочатку він перевіряє кеш (Redis).
3.  **Попадання в кеш**: якщо дані є в кеші, негайно поверніть їх.
4.  **Промах кешу**: якщо даних немає в кеші, отримайте їх з бази даних, збережіть у кеші на наступний раз, а потім поверніть.

```python
from django.core.cache import cache

def get_author_details(author_id):
    cache_key = f"author:{author_id}"
    author = cache.get(cache_key)

    if author is None:
        # Промах кешу
        author = Author.objects.get(pk=author_id)
        cache.set(cache_key, author, timeout=3600) # Кешувати на 1 годину
    
    return author
```

---

## Патерн 2: Обмеження швидкості (Rate Limiting)

**Проблема**: надто багато запитів за короткий час може призвести до блокування вашого скрепера.
**Рішення**: **обмеження швидкості**. Контролюйте швидкість вихідних запитів.

Хоча Scrapy має `DOWNLOAD_DELAY`, вам може знадобитися більш складне обмеження, особливо для API.

### Алгоритм Token Bucket

Простий та ефективний алгоритм для обмеження швидкості, часто реалізований за допомогою Redis.

---

## Патерн 3: Структуроване логування

**Проблема**: журнали у вигляді простого тексту важко шукати та аналізувати.
**Рішення**: **структуроване логування**. Логуйте повідомлення у машиночитаному форматі, такому як JSON.

**Неструктуроване (погано):**
`"Скрапінг завершено. Знайдено 50 елементів."`

**Структуроване (добре):**
```json
{
    "event": "scrape_complete",
    "spider": "quotes_spider",
    "items_found": 50,
    "duration_seconds": 12.5
}
```
Це дозволяє легко фільтрувати, шукати та створювати інформаційні панелі (наприклад, показати середню `duration_seconds` для `quotes_spider`).

---

## Патерн 4: Оптимізація бази даних

**Проблема**: база даних може стати вузьким місцем у міру зростання обсягу даних.
**Рішення**: оптимізуйте взаємодію з базою даних.

- **Використовуйте індекси**: додайте `db_index=True` до полів моделі, за якими ви часто фільтруєте. Це значно прискорює пошук.
- **`select_related` та `prefetch_related`**: уникайте "проблеми N+1 запиту", отримуючи пов'язані об'єкти за один запит.
- **Масові операції**: використовуйте `bulk_create()` та `bulk_update()` для виконання багатьох операцій з базою даних за один запит, що є набагато ефективнішим.

---
## Додаткові ресурси

- [Документація з кешування Django](https://docs.djangoproject.com/en/stable/topics/cache/)
- [Структуроване логування в Python](https://www.datadoghq.com/blog/python-logging-best-practices/)
- [Поради щодо продуктивності бази даних Django](https://docs.djangoproject.com/en/stable/topics/db/optimization/)
