{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Beginner Web Scraping & Parsing (Notebook)\n",
        "\n",
        "A compact, runnable walkthrough that merges the theory and examples from the beginner edition. You can run each cell step by step to see how requests, BeautifulSoup, validation, and persistence fit together.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Path (you can jump into any section)\n",
        "- Foundations: HTTP requests, HTML/CSS structure, respectful scraping\n",
        "- BeautifulSoup essentials: selecting elements, extracting text/attributes\n",
        "- Practical example 1: Quotes scraper with pagination + cleaning\n",
        "- Practical example 2: Books scraper with rating/availability parsing\n",
        "- Practical example 3: Full pipeline with validation and SQLite storage\n",
        "- Next steps and exercises to extend the code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Start\n",
        "1. Create and activate a virtualenv (Python 3.11+ recommended).\n",
        "2. Install deps: `pip install requests beautifulsoup4` (optionally `pandas` for tabular views).\n",
        "3. Run cells from top to bottom. The examples use the legal playground sites `quotes.toscrape.com` and `books.toscrape.com`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Responsible Scraping Checklist\n",
        "- Identify yourself with a User-Agent header.\n",
        "- Respect robots.txt and site terms.\n",
        "- Add short delays between requests (rate limiting).\n",
        "- Handle errors gracefully (timeouts, changed markup).\n",
        "- Validate data before storing; avoid hammering a site with retries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting HTML Quickly\n",
        "A small helper to preview HTML and keep responses on disk for debugging. This is useful when a selector suddenly stops matching because the site changed its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import sqlite3\n",
        "import time\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_html(url: str, *, timeout: int = 10, pause: float = 0.5) -> Optional[str]:\n",
        "    \"\"\"Download HTML with a browser-like header and a small delay.\"\"\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Educational Bot for web scraping practice)\"}\n",
        "    time.sleep(pause)\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.RequestException as exc:\n",
        "        logger.error(\"Error fetching %s: %s\", url, exc)\n",
        "        return None\n",
        "\n",
        "\n",
        "def preview_html(html: str, *, keep: Optional[Path] = None, chars: int = 600) -> str:\n",
        "    \"\"\"Return a short preview of HTML and optionally persist it for offline inspection.\"\"\"\n",
        "    if keep:\n",
        "        keep.parent.mkdir(parents=True, exist_ok=True)\n",
        "        keep.write_text(html, encoding=\"utf-8\")\n",
        "        logger.info(\"Saved raw HTML to %s\", keep)\n",
        "    return html[:chars] + (\"...\" if len(html) > chars else \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BeautifulSoup Basics Recap\n",
        "- `soup.find('tag', class_='name')` returns the first match.\n",
        "- `soup.find_all('div', class_='quote')` returns a list.\n",
        "- Access attributes with `element['href']` or safer `element.get('href')`.\n",
        "- Use `.get_text(strip=True)` to trim whitespace.\n",
        "- Prefer specific selectors; brittle selectors create noisy data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Quotes Scraper (with validation and pagination)\n",
        "We treat each quote as a structured record. The scraper follows the \"Next\" link automatically and cleans duplicates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Quote:\n",
        "    text: str\n",
        "    author: str\n",
        "    tags: List[str]\n",
        "    scraped_at: str\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return asdict(self)\n",
        "\n",
        "\n",
        "def validate_quote(quote: Quote) -> Tuple[bool, Optional[str]]:\n",
        "    if not quote.text or len(quote.text) < 5:\n",
        "        return False, \"Text is missing or too short\"\n",
        "    if not quote.author:\n",
        "        return False, \"Missing author\"\n",
        "    return True, None\n",
        "\n",
        "\n",
        "class QuoteScraper:\n",
        "    def __init__(self, base_url: str = \"http://quotes.toscrape.com\", delay: float = 1.0):\n",
        "        self.base_url = base_url.rstrip(\"/\")\n",
        "        self.delay = delay\n",
        "        self.quotes: List[Quote] = []\n",
        "        self.errors: List[str] = []\n",
        "\n",
        "    def _extract_quotes(self, soup: BeautifulSoup) -> List[Quote]:\n",
        "        items: List[Quote] = []\n",
        "        for block in soup.find_all(\"div\", class_=\"quote\"):\n",
        "            try:\n",
        "                text = block.find(\"span\", class_=\"text\").get_text(strip=True)\n",
        "                author = block.find(\"small\", class_=\"author\").get_text(strip=True)\n",
        "                tags = [tag.get_text(strip=True) for tag in block.find_all(\"a\", class_=\"tag\")]\n",
        "                quote = Quote(text=text, author=author, tags=tags, scraped_at=time.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
        "                is_valid, reason = validate_quote(quote)\n",
        "                if is_valid:\n",
        "                    items.append(quote)\n",
        "                else:\n",
        "                    self.errors.append(f\"Invalid quote: {reason}\")\n",
        "            except AttributeError as exc:\n",
        "                self.errors.append(f\"Parsing error: {exc}\")\n",
        "        return items\n",
        "\n",
        "    def scrape(self, max_pages: int = 3) -> List[Quote]:\n",
        "        url = f\"{self.base_url}/page/1/\"\n",
        "        seen = set()\n",
        "\n",
        "        for page in range(1, max_pages + 1):\n",
        "            logger.info(\"Scraping page %s\", page)\n",
        "            html = fetch_html(url, pause=self.delay)\n",
        "            if not html:\n",
        "                break\n",
        "\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            for quote in self._extract_quotes(soup):\n",
        "                key = (quote.text.lower(), quote.author.lower())\n",
        "                if key not in seen:\n",
        "                    seen.add(key)\n",
        "                    self.quotes.append(quote)\n",
        "\n",
        "            next_link = soup.find(\"li\", class_=\"next\")\n",
        "            if next_link and next_link.find(\"a\"):\n",
        "                next_href = next_link.find(\"a\").get(\"href\")\n",
        "                url = f\"{self.base_url}{next_href}\"\n",
        "            else:\n",
        "                break\n",
        "        return self.quotes\n",
        "\n",
        "    def to_json(self, path: Path) -> None:\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        path.write_text(json.dumps([q.to_dict() for q in self.quotes], indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "        logger.info(\"Saved %s quotes to %s\", len(self.quotes), path)\n",
        "\n",
        "    def preview(self, limit: int = 3) -> None:\n",
        "        for quote in self.quotes[:limit]:\n",
        "            print(f\"— {quote.author}: {quote.text}\")\n",
        "            print(f\"  tags: {', '.join(quote.tags)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the Quotes Scraper\n",
        "Uncomment the cell below to fetch a couple of pages. Data is deduplicated and saved to `data/quotes.json` by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "quote_scraper = QuoteScraper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f3f7b149",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-20 13:16:11,537 - INFO - Scraping page 1\n",
            "2025-12-20 13:16:13,073 - INFO - Scraping page 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "quotes = quote_scraper.scrape(max_pages=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "48c97554",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Quote(text='“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', author='Albert Einstein', tags=['change', 'deep-thoughts', 'thinking', 'world'], scraped_at='2025-12-20T13:16:13'),\n",
              " Quote(text='“It is our choices, Harry, that show what we truly are, far more than our abilities.”', author='J.K. Rowling', tags=['abilities', 'choices'], scraped_at='2025-12-20T13:16:13'),\n",
              " Quote(text='“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', author='Albert Einstein', tags=['inspirational', 'life', 'live', 'miracle', 'miracles'], scraped_at='2025-12-20T13:16:13'),\n",
              " Quote(text='“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', author='Jane Austen', tags=['aliteracy', 'books', 'classic', 'humor'], scraped_at='2025-12-20T13:16:13'),\n",
              " Quote(text=\"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", author='Marilyn Monroe', tags=['be-yourself', 'inspirational'], scraped_at='2025-12-20T13:16:13'),\n",
              " Quote(text='“Try not to become a man of success. Rather become a man of value.”', author='Albert Einstein', tags=['adulthood', 'success', 'value'], scraped_at='2025-12-20T13:16:13'),\n",
              " Quote(text='“It is better to be hated for what you are than to be loved for what you are not.”', author='André Gide', tags=['life', 'love'], scraped_at='2025-12-20T13:16:13'),\n",
              " Quote(text=\"“I have not failed. I've just found 10,000 ways that won't work.”\", author='Thomas A. Edison', tags=['edison', 'failure', 'inspirational', 'paraphrased'], scraped_at='2025-12-20T13:16:13'),\n",
              " Quote(text=\"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", author='Eleanor Roosevelt', tags=['misattributed-eleanor-roosevelt'], scraped_at='2025-12-20T13:16:13'),\n",
              " Quote(text='“A day without sunshine is like, you know, night.”', author='Steve Martin', tags=['humor', 'obvious', 'simile'], scraped_at='2025-12-20T13:16:13'),\n",
              " Quote(text=\"“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\", author='Marilyn Monroe', tags=['friends', 'heartbreak', 'inspirational', 'life', 'love', 'sisters'], scraped_at='2025-12-20T13:16:14'),\n",
              " Quote(text='“It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”', author='J.K. Rowling', tags=['courage', 'friends'], scraped_at='2025-12-20T13:16:14'),\n",
              " Quote(text=\"“If you can't explain it to a six year old, you don't understand it yourself.”\", author='Albert Einstein', tags=['simplicity', 'understand'], scraped_at='2025-12-20T13:16:14'),\n",
              " Quote(text=\"“You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\", author='Bob Marley', tags=['love'], scraped_at='2025-12-20T13:16:14'),\n",
              " Quote(text='“I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”', author='Dr. Seuss', tags=['fantasy'], scraped_at='2025-12-20T13:16:14'),\n",
              " Quote(text='“I may not have gone where I intended to go, but I think I have ended up where I needed to be.”', author='Douglas Adams', tags=['life', 'navigation'], scraped_at='2025-12-20T13:16:14'),\n",
              " Quote(text=\"“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\", author='Elie Wiesel', tags=['activism', 'apathy', 'hate', 'indifference', 'inspirational', 'love', 'opposite', 'philosophy'], scraped_at='2025-12-20T13:16:14'),\n",
              " Quote(text='“It is not a lack of love, but a lack of friendship that makes unhappy marriages.”', author='Friedrich Nietzsche', tags=['friendship', 'lack-of-friendship', 'lack-of-love', 'love', 'marriage', 'unhappy-marriage'], scraped_at='2025-12-20T13:16:14'),\n",
              " Quote(text='“Good friends, good books, and a sleepy conscience: this is the ideal life.”', author='Mark Twain', tags=['books', 'contentment', 'friends', 'friendship', 'life'], scraped_at='2025-12-20T13:16:14'),\n",
              " Quote(text='“Life is what happens to us while we are making other plans.”', author='Allen Saunders', tags=['fate', 'life', 'misattributed-john-lennon', 'planning', 'plans'], scraped_at='2025-12-20T13:16:14')]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "quote_scraper.quotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0af04ff6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-20 13:18:02,246 - INFO - Saved 20 quotes to data/quotes.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "quote_scraper.to_json(Path(\"data/quotes.json\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "85daeac6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "— Albert Einstein: “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
            "  tags: change, deep-thoughts, thinking, world\n",
            "— J.K. Rowling: “It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
            "  tags: abilities, choices\n",
            "— Albert Einstein: “There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
            "  tags: inspirational, life, live, miracle, miracles\n",
            "— Jane Austen: “The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
            "  tags: aliteracy, books, classic, humor\n",
            "— Marilyn Monroe: “Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
            "  tags: be-yourself, inspirational\n"
          ]
        }
      ],
      "source": [
        "\n",
        "quote_scraper.preview(limit=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a50d9875",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 20 quotes; errors: 0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Collected {len(quotes)} quotes; errors: {len(quote_scraper.errors)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Books Scraper (price, availability, rating)\n",
        "Books.toscrape.com exposes pagination and richer fields. We normalize prices and map star ratings to numbers for easier analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "RATING_MAP = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Book:\n",
        "    title: str\n",
        "    price_gbp: float\n",
        "    availability: str\n",
        "    rating: int\n",
        "    scraped_at: str\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return asdict(self)\n",
        "\n",
        "\n",
        "def parse_price(raw: str) -> float:\n",
        "    digits = raw.replace(\"£\", \"\").strip()\n",
        "    try:\n",
        "        return float(digits)\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def parse_rating(tag: Optional[str]) -> int:\n",
        "    if not tag:\n",
        "        return 0\n",
        "    return RATING_MAP.get(tag, 0)\n",
        "\n",
        "\n",
        "class BookScraper:\n",
        "    def __init__(self, base_url: str = \"http://books.toscrape.com\", delay: float = 1.0):\n",
        "        self.base_url = base_url.rstrip(\"/\")\n",
        "        self.delay = delay\n",
        "        self.books: List[Book] = []\n",
        "        self.errors: List[str] = []\n",
        "\n",
        "    def _extract_books(self, soup: BeautifulSoup) -> List[Book]:\n",
        "        results: List[Book] = []\n",
        "        for article in soup.find_all(\"article\", class_=\"product_pod\"):\n",
        "            try:\n",
        "                title = article.find(\"h2\").find(\"a\").get(\"title\")\n",
        "                price = parse_price(article.find(\"p\", class_=\"price_color\").get_text())\n",
        "                availability = article.find(\"p\", class_=\"instock availability\").get_text(strip=True)\n",
        "                rating_tag = article.find(\"p\", class_=\"star-rating\").get(\"class\", [None, None])[1]\n",
        "                book = Book(\n",
        "                    title=title,\n",
        "                    price_gbp=price,\n",
        "                    availability=availability,\n",
        "                    rating=parse_rating(rating_tag),\n",
        "                    scraped_at=time.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
        "                )\n",
        "                results.append(book)\n",
        "            except (AttributeError, IndexError) as exc:\n",
        "                self.errors.append(f\"Parsing error: {exc}\")\n",
        "        return results\n",
        "\n",
        "    def scrape(self, max_pages: int = 2) -> List[Book]:\n",
        "        url = f\"{self.base_url}/catalogue/page-1.html\"\n",
        "        seen = set()\n",
        "\n",
        "        for page in range(1, max_pages + 1):\n",
        "            logger.info(\"Scraping books page %s\", page)\n",
        "            html = fetch_html(url, pause=self.delay)\n",
        "            if not html:\n",
        "                break\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            for book in self._extract_books(soup):\n",
        "                if book.title not in seen:\n",
        "                    seen.add(book.title)\n",
        "                    self.books.append(book)\n",
        "            next_link = soup.find(\"li\", class_=\"next\")\n",
        "            if next_link and next_link.find(\"a\"):\n",
        "                href = next_link.find(\"a\").get(\"href\")\n",
        "                url = f\"{self.base_url}/catalogue/{href}\"\n",
        "            else:\n",
        "                break\n",
        "        return self.books\n",
        "\n",
        "    def to_json(self, path: Path) -> None:\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        path.write_text(json.dumps([b.to_dict() for b in self.books], indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "        logger.info(\"Saved %s books to %s\", len(self.books), path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the Books Scraper\n",
        "Uncomment to gather a few pages of data and store them in `data/books.json`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-20 13:20:33,675 - INFO - Scraping books page 1\n",
            "2025-12-20 13:20:35,008 - INFO - Scraping books page 2\n",
            "2025-12-20 13:20:36,341 - INFO - Saved 0 books to data/books.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 0 books; errors: 40\n"
          ]
        }
      ],
      "source": [
        "book_scraper = BookScraper()\n",
        "books = book_scraper.scrape(max_pages=2)\n",
        "book_scraper.to_json(Path(\"data/books.json\"))\n",
        "print(f\"Collected {len(books)} books; errors: {len(book_scraper.errors)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: End-to-End Pipeline With SQLite\n",
        "The pipeline mirrors a production workflow: fetch → parse → validate → deduplicate → persist. We use SQLite for simplicity and add basic duplicate prevention via a unique URL constraint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class NewsArticle:\n",
        "    title: str\n",
        "    url: str\n",
        "    content: Optional[str]\n",
        "    category: Optional[str]\n",
        "    published_date: Optional[str]\n",
        "    scraped_at: str\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return asdict(self)\n",
        "\n",
        "\n",
        "class NewsDataValidator:\n",
        "    MIN_TITLE = 5\n",
        "    MIN_CONTENT = 10\n",
        "\n",
        "    @classmethod\n",
        "    def validate(cls, article: NewsArticle) -> Tuple[bool, Optional[str]]:\n",
        "        if not article.title or len(article.title) < cls.MIN_TITLE:\n",
        "            return False, \"Title is missing or too short\"\n",
        "        if not article.url:\n",
        "            return False, \"URL is required\"\n",
        "        if article.content and len(article.content) < cls.MIN_CONTENT:\n",
        "            return False, \"Content is too short\"\n",
        "        return True, None\n",
        "\n",
        "    @classmethod\n",
        "    def clean(cls, articles: List[NewsArticle]) -> Tuple[List[NewsArticle], List[str]]:\n",
        "        valid: List[NewsArticle] = []\n",
        "        errors: List[str] = []\n",
        "        seen = set()\n",
        "        for article in articles:\n",
        "            ok, reason = cls.validate(article)\n",
        "            if not ok:\n",
        "                errors.append(f\"Invalid: {reason}\")\n",
        "                continue\n",
        "            key = article.title.lower().strip()\n",
        "            if key in seen:\n",
        "                errors.append(f\"Duplicate title: {article.title}\")\n",
        "                continue\n",
        "            seen.add(key)\n",
        "            valid.append(article)\n",
        "        return valid, errors\n",
        "\n",
        "\n",
        "class NewsDatabase:\n",
        "    def __init__(self, path: Path = Path(\"data/news_demo.db\")):\n",
        "        self.path = path\n",
        "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        self._init()\n",
        "\n",
        "    def _init(self) -> None:\n",
        "        conn = sqlite3.connect(self.path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS articles (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                title TEXT NOT NULL,\n",
        "                url TEXT NOT NULL UNIQUE,\n",
        "                content TEXT,\n",
        "                category TEXT,\n",
        "                published_date TEXT,\n",
        "                scraped_at TEXT NOT NULL,\n",
        "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "            )\n",
        "            \"\"\"\n",
        "        )\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def save(self, articles: List[NewsArticle]) -> int:\n",
        "        conn = sqlite3.connect(self.path)\n",
        "        cursor = conn.cursor()\n",
        "        saved = 0\n",
        "        for item in articles:\n",
        "            try:\n",
        "                cursor.execute(\n",
        "                    \"\"\"\n",
        "                    INSERT INTO articles (title, url, content, category, published_date, scraped_at)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?)\n",
        "                    \"\"\",\n",
        "                    (item.title, item.url, item.content, item.category, item.published_date, item.scraped_at),\n",
        "                )\n",
        "                saved += 1\n",
        "            except sqlite3.IntegrityError:\n",
        "                logger.warning(\"Duplicate URL skipped: %s\", item.url)\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        return saved\n",
        "\n",
        "    def latest(self, limit: int = 5) -> List[NewsArticle]:\n",
        "        conn = sqlite3.connect(self.path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            SELECT title, url, content, category, published_date, scraped_at\n",
        "            FROM articles\n",
        "            ORDER BY created_at DESC\n",
        "            LIMIT ?\n",
        "            \"\"\",\n",
        "            (limit,),\n",
        "        )\n",
        "        rows = cursor.fetchall()\n",
        "        conn.close()\n",
        "        return [\n",
        "            NewsArticle(\n",
        "                title=row[0], url=row[1], content=row[2], category=row[3], published_date=row[4], scraped_at=row[5]\n",
        "            )\n",
        "            for row in rows\n",
        "        ]\n",
        "\n",
        "    def stats(self) -> Dict:\n",
        "        conn = sqlite3.connect(self.path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM articles\")\n",
        "        total = cursor.fetchone()[0]\n",
        "        cursor.execute(\"SELECT COUNT(DISTINCT category) FROM articles\")\n",
        "        categories = cursor.fetchone()[0]\n",
        "        conn.close()\n",
        "        return {\"total\": total, \"categories\": categories}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline Runner (Quotes as pseudo-news)\n",
        "For a quick demo we reuse `quotes.toscrape.com` as the source. Swap the selectors to adapt to a real news site.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-20 13:21:28,266 - INFO - [news] page 1\n",
            "2025-12-20 13:21:29,658 - INFO - [news] page 2\n",
            "2025-12-20 13:21:30,871 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/1/\n",
            "2025-12-20 13:21:30,872 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/1/\n",
            "2025-12-20 13:21:30,872 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/1/\n",
            "2025-12-20 13:21:30,872 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/1/\n",
            "2025-12-20 13:21:30,873 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/1/\n",
            "2025-12-20 13:21:30,873 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/1/\n",
            "2025-12-20 13:21:30,875 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/1/\n",
            "2025-12-20 13:21:30,876 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/2/\n",
            "2025-12-20 13:21:30,876 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/2/\n",
            "2025-12-20 13:21:30,878 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/2/\n",
            "2025-12-20 13:21:30,879 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/2/\n",
            "2025-12-20 13:21:30,879 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/2/\n",
            "2025-12-20 13:21:30,880 - WARNING - Duplicate URL skipped: http://quotes.toscrape.com/page/2/\n",
            "2025-12-20 13:21:30,883 - INFO - Saved 2 articles; 5 validation errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Albert Einstein: “The world as we have created it is a process of our thinking. It cannot be chan...\n",
            "- Bob Marley: “You may not be her first, her last, or her only. She loved before she may love ...\n",
            "Stats: {'total': 2, 'categories': 1}\n"
          ]
        }
      ],
      "source": [
        "class NewsPortalScraper:\n",
        "    def __init__(self, base_url: str = \"http://quotes.toscrape.com\", delay: float = 1.0):\n",
        "        self.base_url = base_url.rstrip(\"/\")\n",
        "        self.delay = delay\n",
        "\n",
        "    def scrape(self, pages: int = 2) -> List[NewsArticle]:\n",
        "        items: List[NewsArticle] = []\n",
        "        url = f\"{self.base_url}/page/1/\"\n",
        "        for page in range(1, pages + 1):\n",
        "            logger.info(\"[news] page %s\", page)\n",
        "            html = fetch_html(url, pause=self.delay)\n",
        "            if not html:\n",
        "                break\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            for block in soup.find_all(\"div\", class_=\"quote\"):\n",
        "                text = block.find(\"span\", class_=\"text\").get_text(strip=True)\n",
        "                author = block.find(\"small\", class_=\"author\").get_text(strip=True)\n",
        "                article = NewsArticle(\n",
        "                    title=author,\n",
        "                    url=url,\n",
        "                    content=text,\n",
        "                    category=\"quotes\",\n",
        "                    published_date=None,\n",
        "                    scraped_at=time.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
        "                )\n",
        "                items.append(article)\n",
        "            next_link = soup.find(\"li\", class_=\"next\")\n",
        "            if next_link and next_link.find(\"a\"):\n",
        "                url = f\"{self.base_url}{next_link.find('a').get('href')}\"\n",
        "            else:\n",
        "                break\n",
        "        return items\n",
        "\n",
        "\n",
        "def run_pipeline(pages: int = 2):\n",
        "    scraper = NewsPortalScraper()\n",
        "    raw_articles = scraper.scrape(pages=pages)\n",
        "    valid_articles, errors = NewsDataValidator.clean(raw_articles)\n",
        "    db = NewsDatabase()\n",
        "    saved = db.save(valid_articles)\n",
        "    latest = db.latest(limit=3)\n",
        "    stats = db.stats()\n",
        "    logger.info(\"Saved %s articles; %s validation errors\", saved, len(errors))\n",
        "    for article in latest:\n",
        "        print(f\"- {article.title}: {article.content[:80]}...\")\n",
        "    print(\"Stats:\", stats)\n",
        "\n",
        "# Uncomment to run the pipeline\n",
        "run_pipeline(pages=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps & Exercises\n",
        "- Extend the books scraper to export CSV alongside JSON.\n",
        "- Add search and filtering helpers (e.g., by tag or price range).\n",
        "- Swap in a real news site: update selectors, add `published_date` parsing, and improve duplicate checks (title + date).\n",
        "- Introduce proxy rotation and exponential backoff for tougher targets.\n",
        "- Pair this pipeline with a Django management command to populate a database and render the data in templates.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
