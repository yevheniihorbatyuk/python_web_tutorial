{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Beginner Web Scraping & Parsing (Notebook)\n\nA compact, runnable walkthrough that merges the theory and examples from the beginner edition. You can run each cell step by step to see how requests, BeautifulSoup, validation, and persistence fit together.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Learning Path (you can jump into any section)\n- Foundations: HTTP requests, HTML/CSS structure, respectful scraping\n- BeautifulSoup essentials: selecting elements, extracting text/attributes\n- Practical example 1: Quotes scraper with pagination + cleaning\n- Practical example 2: Books scraper with rating/availability parsing\n- Practical example 3: Full pipeline with validation and SQLite storage\n- Next steps and exercises to extend the code\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Quick Start\n1. Create and activate a virtualenv (Python 3.11+ recommended).\n2. Install deps: `pip install requests beautifulsoup4` (optionally `pandas` for tabular views).\n3. Run cells from top to bottom. The examples use the legal playground sites `quotes.toscrape.com` and `books.toscrape.com`.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Responsible Scraping Checklist\n- Identify yourself with a User-Agent header.\n- Respect robots.txt and site terms.\n- Add short delays between requests (rate limiting).\n- Handle errors gracefully (timeouts, changed markup).\n- Validate data before storing; avoid hammering a site with retries.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Inspecting HTML Quickly\nA small helper to preview HTML and keep responses on disk for debugging. This is useful when a selector suddenly stops matching because the site changed its structure.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from __future__ import annotations\n\nimport json\nimport logging\nimport sqlite3\nimport time\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def fetch_html(url: str, *, timeout: int = 10, pause: float = 0.5) -> Optional[str]:\n    \"\"\"Download HTML with a browser-like header and a small delay.\"\"\"\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Educational Bot for web scraping practice)\"}\n    time.sleep(pause)\n    try:\n        response = requests.get(url, headers=headers, timeout=timeout)\n        response.raise_for_status()\n        return response.text\n    except requests.RequestException as exc:\n        logger.error(\"Error fetching %s: %s\", url, exc)\n        return None\n\n\ndef preview_html(html: str, *, keep: Optional[Path] = None, chars: int = 600) -> str:\n    \"\"\"Return a short preview of HTML and optionally persist it for offline inspection.\"\"\"\n    if keep:\n        keep.parent.mkdir(parents=True, exist_ok=True)\n        keep.write_text(html, encoding=\"utf-8\")\n        logger.info(\"Saved raw HTML to %s\", keep)\n    return html[:chars] + (\"...\" if len(html) > chars else \"\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## BeautifulSoup Basics Recap\n- `soup.find('tag', class_='name')` returns the first match.\n- `soup.find_all('div', class_='quote')` returns a list.\n- Access attributes with `element['href']` or safer `element.get('href')`.\n- Use `.get_text(strip=True)` to trim whitespace.\n- Prefer specific selectors; brittle selectors create noisy data.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Example 1: Quotes Scraper (with validation and pagination)\nWe treat each quote as a structured record. The scraper follows the \"Next\" link automatically and cleans duplicates.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@dataclass\nclass Quote:\n    text: str\n    author: str\n    tags: List[str]\n    scraped_at: str\n\n    def to_dict(self) -> Dict:\n        return asdict(self)\n\n\ndef validate_quote(quote: Quote) -> Tuple[bool, Optional[str]]:\n    if not quote.text or len(quote.text) < 5:\n        return False, \"Text is missing or too short\"\n    if not quote.author:\n        return False, \"Missing author\"\n    return True, None\n\n\nclass QuoteScraper:\n    def __init__(self, base_url: str = \"http://quotes.toscrape.com\", delay: float = 1.0):\n        self.base_url = base_url.rstrip(\"/\")\n        self.delay = delay\n        self.quotes: List[Quote] = []\n        self.errors: List[str] = []\n\n    def _extract_quotes(self, soup: BeautifulSoup) -> List[Quote]:\n        items: List[Quote] = []\n        for block in soup.find_all(\"div\", class_=\"quote\"):\n            try:\n                text = block.find(\"span\", class_=\"text\").get_text(strip=True)\n                author = block.find(\"small\", class_=\"author\").get_text(strip=True)\n                tags = [tag.get_text(strip=True) for tag in block.find_all(\"a\", class_=\"tag\")]\n                quote = Quote(text=text, author=author, tags=tags, scraped_at=time.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n                is_valid, reason = validate_quote(quote)\n                if is_valid:\n                    items.append(quote)\n                else:\n                    self.errors.append(f\"Invalid quote: {reason}\")\n            except AttributeError as exc:\n                self.errors.append(f\"Parsing error: {exc}\")\n        return items\n\n    def scrape(self, max_pages: int = 3) -> List[Quote]:\n        url = f\"{self.base_url}/page/1/\"\n        seen = set()\n\n        for page in range(1, max_pages + 1):\n            logger.info(\"Scraping page %s\", page)\n            html = fetch_html(url, pause=self.delay)\n            if not html:\n                break\n\n            soup = BeautifulSoup(html, \"html.parser\")\n            for quote in self._extract_quotes(soup):\n                key = (quote.text.lower(), quote.author.lower())\n                if key not in seen:\n                    seen.add(key)\n                    self.quotes.append(quote)\n\n            next_link = soup.find(\"li\", class_=\"next\")\n            if next_link and next_link.find(\"a\"):\n                next_href = next_link.find(\"a\").get(\"href\")\n                url = f\"{self.base_url}{next_href}\"\n            else:\n                break\n        return self.quotes\n\n    def to_json(self, path: Path) -> None:\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(json.dumps([q.to_dict() for q in self.quotes], indent=2, ensure_ascii=False), encoding=\"utf-8\")\n        logger.info(\"Saved %s quotes to %s\", len(self.quotes), path)\n\n    def preview(self, limit: int = 3) -> None:\n        for quote in self.quotes[:limit]:\n            print(f\"— {quote.author}: {quote.text}\")\n            print(f\"  tags: {', '.join(quote.tags)}\n\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Run the Quotes Scraper\nUncomment the cell below to fetch a couple of pages. Data is deduplicated and saved to `data/quotes.json` by default.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# quote_scraper = QuoteScraper()\n# quotes = quote_scraper.scrape(max_pages=2)\n# quote_scraper.to_json(Path(\"data/quotes.json\"))\n# quote_scraper.preview(limit=5)\n# print(f\"Collected {len(quotes)} quotes; errors: {len(quote_scraper.errors)}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Example 2: Books Scraper (price, availability, rating)\nBooks.toscrape.com exposes pagination and richer fields. We normalize prices and map star ratings to numbers for easier analysis.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "RATING_MAP = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n\n\n@dataclass\nclass Book:\n    title: str\n    price_gbp: float\n    availability: str\n    rating: int\n    scraped_at: str\n\n    def to_dict(self) -> Dict:\n        return asdict(self)\n\n\ndef parse_price(raw: str) -> float:\n    digits = raw.replace(\"£\", \"\").strip()\n    try:\n        return float(digits)\n    except ValueError:\n        return 0.0\n\n\ndef parse_rating(tag: Optional[str]) -> int:\n    if not tag:\n        return 0\n    return RATING_MAP.get(tag, 0)\n\n\nclass BookScraper:\n    def __init__(self, base_url: str = \"http://books.toscrape.com\", delay: float = 1.0):\n        self.base_url = base_url.rstrip(\"/\")\n        self.delay = delay\n        self.books: List[Book] = []\n        self.errors: List[str] = []\n\n    def _extract_books(self, soup: BeautifulSoup) -> List[Book]:\n        results: List[Book] = []\n        for article in soup.find_all(\"article\", class_=\"product_pod\"):\n            try:\n                title = article.find(\"h2\").find(\"a\").get(\"title\")\n                price = parse_price(article.find(\"p\", class_=\"price_color\").get_text())\n                availability = article.find(\"p\", class_=\"instock availability\").get_text(strip=True)\n                rating_tag = article.find(\"p\", class_=\"star-rating\").get(\"class\", [None, None])[1]\n                book = Book(\n                    title=title,\n                    price_gbp=price,\n                    availability=availability,\n                    rating=parse_rating(rating_tag),\n                    scraped_at=time.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n                )\n                results.append(book)\n            except (AttributeError, IndexError) as exc:\n                self.errors.append(f\"Parsing error: {exc}\")\n        return results\n\n    def scrape(self, max_pages: int = 2) -> List[Book]:\n        url = f\"{self.base_url}/catalogue/page-1.html\"\n        seen = set()\n\n        for page in range(1, max_pages + 1):\n            logger.info(\"Scraping books page %s\", page)\n            html = fetch_html(url, pause=self.delay)\n            if not html:\n                break\n            soup = BeautifulSoup(html, \"html.parser\")\n            for book in self._extract_books(soup):\n                if book.title not in seen:\n                    seen.add(book.title)\n                    self.books.append(book)\n            next_link = soup.find(\"li\", class_=\"next\")\n            if next_link and next_link.find(\"a\"):\n                href = next_link.find(\"a\").get(\"href\")\n                url = f\"{self.base_url}/catalogue/{href}\"\n            else:\n                break\n        return self.books\n\n    def to_json(self, path: Path) -> None:\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(json.dumps([b.to_dict() for b in self.books], indent=2, ensure_ascii=False), encoding=\"utf-8\")\n        logger.info(\"Saved %s books to %s\", len(self.books), path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Run the Books Scraper\nUncomment to gather a few pages of data and store them in `data/books.json`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# book_scraper = BookScraper()\n# books = book_scraper.scrape(max_pages=2)\n# book_scraper.to_json(Path(\"data/books.json\"))\n# print(f\"Collected {len(books)} books; errors: {len(book_scraper.errors)}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Example 3: End-to-End Pipeline With SQLite\nThe pipeline mirrors a production workflow: fetch → parse → validate → deduplicate → persist. We use SQLite for simplicity and add basic duplicate prevention via a unique URL constraint.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@dataclass\nclass NewsArticle:\n    title: str\n    url: str\n    content: Optional[str]\n    category: Optional[str]\n    published_date: Optional[str]\n    scraped_at: str\n\n    def to_dict(self) -> Dict:\n        return asdict(self)\n\n\nclass NewsDataValidator:\n    MIN_TITLE = 5\n    MIN_CONTENT = 10\n\n    @classmethod\n    def validate(cls, article: NewsArticle) -> Tuple[bool, Optional[str]]:\n        if not article.title or len(article.title) < cls.MIN_TITLE:\n            return False, \"Title is missing or too short\"\n        if not article.url:\n            return False, \"URL is required\"\n        if article.content and len(article.content) < cls.MIN_CONTENT:\n            return False, \"Content is too short\"\n        return True, None\n\n    @classmethod\n    def clean(cls, articles: List[NewsArticle]) -> Tuple[List[NewsArticle], List[str]]:\n        valid: List[NewsArticle] = []\n        errors: List[str] = []\n        seen = set()\n        for article in articles:\n            ok, reason = cls.validate(article)\n            if not ok:\n                errors.append(f\"Invalid: {reason}\")\n                continue\n            key = article.title.lower().strip()\n            if key in seen:\n                errors.append(f\"Duplicate title: {article.title}\")\n                continue\n            seen.add(key)\n            valid.append(article)\n        return valid, errors\n\n\nclass NewsDatabase:\n    def __init__(self, path: Path = Path(\"data/news_demo.db\")):\n        self.path = path\n        self.path.parent.mkdir(parents=True, exist_ok=True)\n        self._init()\n\n    def _init(self) -> None:\n        conn = sqlite3.connect(self.path)\n        cursor = conn.cursor()\n        cursor.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS articles (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                title TEXT NOT NULL,\n                url TEXT NOT NULL UNIQUE,\n                content TEXT,\n                category TEXT,\n                published_date TEXT,\n                scraped_at TEXT NOT NULL,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            )\n            \"\"\"\n        )\n        conn.commit()\n        conn.close()\n\n    def save(self, articles: List[NewsArticle]) -> int:\n        conn = sqlite3.connect(self.path)\n        cursor = conn.cursor()\n        saved = 0\n        for item in articles:\n            try:\n                cursor.execute(\n                    \"\"\"\n                    INSERT INTO articles (title, url, content, category, published_date, scraped_at)\n                    VALUES (?, ?, ?, ?, ?, ?)\n                    \"\"\",\n                    (item.title, item.url, item.content, item.category, item.published_date, item.scraped_at),\n                )\n                saved += 1\n            except sqlite3.IntegrityError:\n                logger.warning(\"Duplicate URL skipped: %s\", item.url)\n        conn.commit()\n        conn.close()\n        return saved\n\n    def latest(self, limit: int = 5) -> List[NewsArticle]:\n        conn = sqlite3.connect(self.path)\n        cursor = conn.cursor()\n        cursor.execute(\n            \"\"\"\n            SELECT title, url, content, category, published_date, scraped_at\n            FROM articles\n            ORDER BY created_at DESC\n            LIMIT ?\n            \"\"\",\n            (limit,),\n        )\n        rows = cursor.fetchall()\n        conn.close()\n        return [\n            NewsArticle(\n                title=row[0], url=row[1], content=row[2], category=row[3], published_date=row[4], scraped_at=row[5]\n            )\n            for row in rows\n        ]\n\n    def stats(self) -> Dict:\n        conn = sqlite3.connect(self.path)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT COUNT(*) FROM articles\")\n        total = cursor.fetchone()[0]\n        cursor.execute(\"SELECT COUNT(DISTINCT category) FROM articles\")\n        categories = cursor.fetchone()[0]\n        conn.close()\n        return {\"total\": total, \"categories\": categories}\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Pipeline Runner (Quotes as pseudo-news)\nFor a quick demo we reuse `quotes.toscrape.com` as the source. Swap the selectors to adapt to a real news site.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class NewsPortalScraper:\n    def __init__(self, base_url: str = \"http://quotes.toscrape.com\", delay: float = 1.0):\n        self.base_url = base_url.rstrip(\"/\")\n        self.delay = delay\n\n    def scrape(self, pages: int = 2) -> List[NewsArticle]:\n        items: List[NewsArticle] = []\n        url = f\"{self.base_url}/page/1/\"\n        for page in range(1, pages + 1):\n            logger.info(\"[news] page %s\", page)\n            html = fetch_html(url, pause=self.delay)\n            if not html:\n                break\n            soup = BeautifulSoup(html, \"html.parser\")\n            for block in soup.find_all(\"div\", class_=\"quote\"):\n                text = block.find(\"span\", class_=\"text\").get_text(strip=True)\n                author = block.find(\"small\", class_=\"author\").get_text(strip=True)\n                article = NewsArticle(\n                    title=author,\n                    url=url,\n                    content=text,\n                    category=\"quotes\",\n                    published_date=None,\n                    scraped_at=time.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n                )\n                items.append(article)\n            next_link = soup.find(\"li\", class_=\"next\")\n            if next_link and next_link.find(\"a\"):\n                url = f\"{self.base_url}{next_link.find('a').get('href')}\"\n            else:\n                break\n        return items\n\n\ndef run_pipeline(pages: int = 2):\n    scraper = NewsPortalScraper()\n    raw_articles = scraper.scrape(pages=pages)\n    valid_articles, errors = NewsDataValidator.clean(raw_articles)\n    db = NewsDatabase()\n    saved = db.save(valid_articles)\n    latest = db.latest(limit=3)\n    stats = db.stats()\n    logger.info(\"Saved %s articles; %s validation errors\", saved, len(errors))\n    for article in latest:\n        print(f\"- {article.title}: {article.content[:80]}...\")\n    print(\"Stats:\", stats)\n\n# Uncomment to run the pipeline\n# run_pipeline(pages=2)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Next Steps & Exercises\n- Extend the books scraper to export CSV alongside JSON.\n- Add search and filtering helpers (e.g., by tag or price range).\n- Swap in a real news site: update selectors, add `published_date` parsing, and improve duplicate checks (title + date).\n- Introduce proxy rotation and exponential backoff for tougher targets.\n- Pair this pipeline with a Django management command to populate a database and render the data in templates.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}