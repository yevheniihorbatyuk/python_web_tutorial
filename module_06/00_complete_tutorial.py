# %% [markdown]
#  # üêç Python Web - Module 6: Complete Tutorial
# 
#  ## –†–µ–ª—è—Ü—ñ–π–Ω—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö —Ç–∞ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è
# 
# 
# 
#  **–í–µ—Ä—Å—ñ—è**: 2.0.0
# 
#  **–†—ñ–≤–Ω—ñ**: Basic + Advanced
# 
#  **–¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å**: 5-7 –≥–æ–¥–∏–Ω
# 
# 
# 
#  ---
# 
# 
# 
#  ## üìã –ó–º—ñ—Å—Ç
# 
# 
# 
#  ### –ß–∞—Å—Ç–∏–Ω–∞ 1: –ë–∞–∑–æ–≤—ñ –ú–æ–¥—É–ª—ñ (3-4 –≥–æ–¥–∏–Ω–∏)
# 
#  1. **–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –ü—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è**
# 
#     - Event Loop —Ç–∞ async/await
# 
#     - –ü–∞—Ä–∞–ª–µ–ª—å–Ω—ñ HTTP –∑–∞–ø–∏—Ç–∏ –∑ aiohttp
# 
#     - –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è sync vs async
# 
# 
# 
#  2. **SQL –û—Å–Ω–æ–≤–∏**
# 
#     - SELECT, JOIN, GROUP BY
# 
#     - Window Functions
# 
#     - Subqueries —Ç–∞ CTEs
# 
# 
# 
#  3. **Python + PostgreSQL**
# 
#     - –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è —á–µ—Ä–µ–∑ psycopg2
# 
#     - CRUD –æ–ø–µ—Ä–∞—Ü—ñ—ó
# 
#     - –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó —Ç–∞ –±–µ–∑–ø–µ–∫–∞
# 
# 
# 
#  4. **–ê–Ω–∞–ª—ñ–∑ –î–∞–Ω–∏—Ö**
# 
#     - pandas + PostgreSQL
# 
#     - RFM —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è
# 
#     - –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
# 
# 
# 
#  ### –ß–∞—Å—Ç–∏–Ω–∞ 2: Advanced –ú–æ–¥—É–ª—ñ (2-3 –≥–æ–¥–∏–Ω–∏)
# 
#  5. **Production ETL Pipeline**
# 
#     - Async extraction –∑ –º–Ω–æ–∂–∏–Ω–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª
# 
#     - Data validation —Ç–∞ type safety
# 
#     - Metrics collection
# 
# 
# 
#  6. **Architectural Patterns**
# 
#     - Repository Pattern
# 
#     - Dependency Injection
# 
#     - Factory Pattern
# 
# 
# 
#  7. **ML Feature Store**
# 
#     - Feature engineering
# 
#     - Offline/Online stores
# 
#     - ML infrastructure
# 
# 
# 
#  8. **Advanced SQL Analytics**
# 
#     - Cohort analysis
# 
#     - Funnel analysis
# 
#     - Time-series analysis
# 
# 
# 
#  ---

# %% [markdown]
#  ## ‚öôÔ∏è –ü–æ—á–∞—Ç–∫–æ–≤–∞ –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
# 
# 
# 
#  –ü–µ—Ä–µ–¥ –ø–æ—á–∞—Ç–∫–æ–º –ø–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è —â–æ:
# 
#  - ‚úÖ Docker Desktop –∑–∞–ø—É—â–µ–Ω–æ
# 
#  - ‚úÖ PostgreSQL –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –ø—Ä–∞—Ü—é—î
# 
#  - ‚úÖ –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤—Å—ñ Python –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ

# %%
# –Ü–º–ø–æ—Ä—Ç–∏ –¥–ª—è –≤—Å—å–æ–≥–æ notebook
import sys
import os
import asyncio
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import warnings

# –î–æ–¥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–æ —à–ª—è—Ö—É Python
sys.path.insert(0, os.path.abspath('.'))

# –í–∏–º–∏–∫–∞—î–º–æ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è
warnings.filterwarnings('ignore')

print("‚úÖ –Ü–º–ø–æ—Ä—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")


# %% [markdown]
#  ### –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ Docker —Ç–∞ PostgreSQL

# %%
import subprocess

def check_docker():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ –∑–∞–ø—É—â–µ–Ω–æ Docker"""
    try:
        result = subprocess.run(['docker', 'ps'],
                              capture_output=True,
                              text=True,
                              timeout=5)
        if result.returncode == 0:
            print("‚úÖ Docker –∑–∞–ø—É—â–µ–Ω–æ")

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ PostgreSQL –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
            if 'postgres' in result.stdout:
                print("‚úÖ PostgreSQL –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –ø—Ä–∞—Ü—é—î")
                return True
            else:
                print("‚ö†Ô∏è PostgreSQL –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                print("üí° –ó–∞–ø—É—Å—Ç—ñ—Ç—å: docker-compose up -d")
                return False
        else:
            print("‚ùå Docker –Ω–µ –∑–∞–ø—É—â–µ–Ω–æ")
            return False
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ Docker: {e}")
        return False

docker_ok = check_docker()


# %% [markdown]
#  ### –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ PostgreSQL

# %%
import psycopg2
from psycopg2.extras import RealDictCursor

def test_db_connection():
    """–¢–µ—Å—Ç –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"""
    try:
        conn = psycopg2.connect(
            host="localhost",
            port=5432,
            database="learning_db",
            user="admin",
            password="admin123"
        )

        cursor = conn.cursor()
        cursor.execute("SELECT version();")
        version = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM customers;")
        customer_count = cursor.fetchone()[0]

        cursor.close()
        conn.close()

        print("‚úÖ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ PostgreSQL —É—Å–ø—ñ—à–Ω–µ")
        print(f"üìä PostgreSQL –≤–µ—Ä—Å—ñ—è: {version[:50]}...")
        print(f"üë• –ö–ª—ñ—î–Ω—Ç—ñ–≤ –≤ –ë–î: {customer_count}")
        return True

    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è: {e}")
        print("üí° –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—å —â–æ PostgreSQL –∑–∞–ø—É—â–µ–Ω–æ: docker-compose up -d")
        return False

if docker_ok:
    db_ok = test_db_connection()
else:
    print("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É –ë–î (Docker –Ω–µ –∑–∞–ø—É—â–µ–Ω–æ)")
    db_ok = False


# %% [markdown]
#  ---
# 
#  # –ß–∞—Å—Ç–∏–Ω–∞ 1: –ë–∞–∑–æ–≤—ñ –ú–æ–¥—É–ª—ñ
# 
#  ---

# %% [markdown]
#  ## 1Ô∏è‚É£ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –ü—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è
# 
# 
# 
#  ### üéØ –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ:
# 
#  - –ó—Ä–æ–∑—É–º—ñ—Ç–∏ —è–∫ –ø—Ä–∞—Ü—é—î Event Loop
# 
#  - –ù–∞–≤—á–∏—Ç–∏—Å—è –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ async/await
# 
#  - –ü–æ–±–∞—á–∏—Ç–∏ —Ä—ñ–∑–Ω–∏—Ü—é –≤ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ sync vs async
# 
#  - –ü–∏—Å–∞—Ç–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω—ñ HTTP –∑–∞–ø–∏—Ç–∏ –∑ aiohttp

# %% [markdown]
#  ### 1.1 Event Loop —Ç–∞ async/await - –û—Å–Ω–æ–≤–∏
# 
# 
# 
#  **Event Loop** - —Ü–µ —Å–µ—Ä—Ü–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –≤ Python. –í—ñ–Ω:
# 
#  - –ö–µ—Ä—É—î –≤–∏–∫–æ–Ω–∞–Ω–Ω—è–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏—Ö –∑–∞–¥–∞—á
# 
#  - –ü–µ—Ä–µ–∫–ª—é—á–∞—î—Ç—å—Å—è –º—ñ–∂ –∑–∞–¥–∞—á–∞–º–∏ –∫–æ–ª–∏ –≤–æ–Ω–∏ –æ—á—ñ–∫—É—é—Ç—å (I/O –æ–ø–µ—Ä–∞—Ü—ñ—ó)
# 
#  - –î–æ–∑–≤–æ–ª—è—î –ø–∞—Ä–∞–ª–µ–ª—ñ–∑–º –±–µ–∑ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Ç–æ–∫—ñ–≤

# %%
# –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: sync vs async

def sync_task(name: str, duration: int):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –∑–∞–¥–∞—á–∞ - –±–ª–æ–∫—É—î –≤–∏–∫–æ–Ω–∞–Ω–Ω—è"""
    print(f"üîµ Sync Task {name} –ø–æ—á–∞–ª–∞—Å—å")
    time.sleep(duration)
    print(f"‚úÖ Sync Task {name} –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å ({duration}s)")
    return f"Result {name}"

async def async_task(name: str, duration: int):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –∑–∞–¥–∞—á–∞ - –Ω–µ –±–ª–æ–∫—É—î –≤–∏–∫–æ–Ω–∞–Ω–Ω—è"""
    print(f"üü¢ Async Task {name} –ø–æ—á–∞–ª–∞—Å—å")
    await asyncio.sleep(duration)
    print(f"‚úÖ Async Task {name} –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å ({duration}s)")
    return f"Result {name}"

# Sync –≤–∏–∫–æ–Ω–∞–Ω–Ω—è
print("=" * 60)
print("–°–ò–ù–•–†–û–ù–ù–ï –í–ò–ö–û–ù–ê–ù–ù–Ø (–ø–æ—Å–ª—ñ–¥–æ–≤–Ω–µ)")
print("=" * 60)
start = time.time()
sync_task("A", 1)
sync_task("B", 1)
sync_task("C", 1)
sync_total = time.time() - start
print(f"\n‚è±Ô∏è –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {sync_total:.2f}s\n")

# Async –≤–∏–∫–æ–Ω–∞–Ω–Ω—è
print("=" * 60)
print("–ê–°–ò–ù–•–†–û–ù–ù–ï –í–ò–ö–û–ù–ê–ù–ù–Ø (–ø–∞—Ä–∞–ª–µ–ª—å–Ω–µ)")
print("=" * 60)
start = time.time()
await asyncio.gather(
    async_task("A", 1),
    async_task("B", 1),
    async_task("C", 1)
)
async_total = time.time() - start
print(f"\n‚è±Ô∏è –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {async_total:.2f}s")

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
print("\n" + "=" * 60)
print("–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ü–†–û–î–£–ö–¢–ò–í–ù–û–°–¢–Ü")
print("=" * 60)
print(f"Sync:  {sync_total:.2f}s")
print(f"Async: {async_total:.2f}s")
print(f"üöÄ –ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è: {sync_total/async_total:.1f}x")


# %% [markdown]
#  ### 1.2 –ü–∞—Ä–∞–ª–µ–ª—å–Ω—ñ HTTP –ó–∞–ø–∏—Ç–∏
# 
# 
# 
#  –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ—Å—Ç—å –Ω–∞–π–±—ñ–ª—å—à –∫–æ—Ä–∏—Å–Ω–∞ –¥–ª—è I/O –æ–ø–µ—Ä–∞—Ü—ñ–π, —Ç–∞–∫–∏—Ö —è–∫ HTTP –∑–∞–ø–∏—Ç–∏.
# 
#  –î–∞–≤–∞–π—Ç–µ —ñ–º–ø–æ—Ä—Ç—É—î–º–æ –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –Ω–∞—à–æ–≥–æ –º–æ–¥—É–ª—è.

# %%
# –Ü–º–ø–æ—Ä—Ç—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—ó –∑ async_examples/02_async_http_client.py
import aiohttp
import requests

async def fetch_url_async(session: aiohttp.ClientSession, url: str) -> tuple:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è URL"""
    try:
        start = time.time()
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
            content = await response.text()
            elapsed = time.time() - start
            return url, len(content), elapsed, None
    except Exception as e:
        return url, 0, 0, str(e)

def fetch_url_sync(url: str) -> tuple:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è URL"""
    try:
        start = time.time()
        response = requests.get(url, timeout=10)
        content = response.text
        elapsed = time.time() - start
        return url, len(content), elapsed, None
    except Exception as e:
        return url, 0, 0, str(e)

# –¢–µ—Å—Ç–æ–≤—ñ URLs
urls = [
    "https://httpbin.org/delay/1",
    "https://httpbin.org/delay/1",
    "https://httpbin.org/delay/1",
]

print("=" * 80)
print("–¢–ï–°–¢: –ü–∞—Ä–∞–ª–µ–ª—å–Ω—ñ HTTP –ó–∞–ø–∏—Ç–∏")
print("=" * 80)

# Sync –ø—ñ–¥—Ö—ñ–¥
print("\n1Ô∏è‚É£ –°–ò–ù–•–†–û–ù–ù–ò–ô –ü–Ü–î–•–Ü–î (requests)")
print("-" * 80)
start = time.time()
sync_results = [fetch_url_sync(url) for url in urls]
sync_time = time.time() - start

for url, size, elapsed, error in sync_results:
    if error:
        print(f"‚ùå {url}: {error}")
    else:
        print(f"‚úÖ {url}: {size:,} bytes in {elapsed:.2f}s")
print(f"\n‚è±Ô∏è –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å (sync): {sync_time:.2f}s")

# Async –ø—ñ–¥—Ö—ñ–¥
print("\n2Ô∏è‚É£ –ê–°–ò–ù–•–†–û–ù–ù–ò–ô –ü–Ü–î–•–Ü–î (aiohttp)")
print("-" * 80)
start = time.time()
async with aiohttp.ClientSession() as session:
    tasks = [fetch_url_async(session, url) for url in urls]
    async_results = await asyncio.gather(*tasks)
async_time = time.time() - start

for url, size, elapsed, error in async_results:
    if error:
        print(f"‚ùå {url}: {error}")
    else:
        print(f"‚úÖ {url}: {size:,} bytes in {elapsed:.2f}s")
print(f"\n‚è±Ô∏è –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å (async): {async_time:.2f}s")

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
print("\n" + "=" * 80)
print("–†–ï–ó–£–õ–¨–¢–ê–¢")
print("=" * 80)
print(f"Sync:  {sync_time:.2f}s")
print(f"Async: {async_time:.2f}s")
print(f"üöÄ –ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è: {sync_time/async_time:.1f}x")


# %% [markdown]
#  ### üìä –í–∏—Å–Ω–æ–≤–∫–∏ –∑ Async Programming
# 
# 
# 
#  **–©–æ –º–∏ –Ω–∞–≤—á–∏–ª–∏—Å—è:**
# 
#  - ‚úÖ Event Loop –¥–æ–∑–≤–æ–ª—è—î –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –∑–∞–¥–∞—á—ñ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
# 
#  - ‚úÖ `async/await` - —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π
# 
#  - ‚úÖ `asyncio.gather()` - –≤–∏–∫–æ–Ω—É—î –¥–µ–∫—ñ–ª—å–∫–∞ –∑–∞–¥–∞—á –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
# 
#  - ‚úÖ aiohttp - async –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ requests
# 
#  - ‚úÖ –ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è 3-10x –¥–ª—è I/O –æ–ø–µ—Ä–∞—Ü—ñ–π
# 
# 
# 
#  **–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ async:**
# 
#  - ‚úÖ HTTP –∑–∞–ø–∏—Ç–∏ –¥–æ APIs
# 
#  - ‚úÖ –†–æ–±–æ—Ç–∞ –∑ –±–∞–∑–∞–º–∏ –¥–∞–Ω–∏—Ö
# 
#  - ‚úÖ –§–∞–π–ª–æ–≤—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó
# 
#  - ‚úÖ WebSockets
# 
# 
# 
#  **–ö–æ–ª–∏ –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
# 
#  - ‚ùå CPU-intensive –∑–∞–¥–∞—á—ñ (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ multiprocessing)
# 
#  - ‚ùå –ü—Ä–æ—Å—Ç—ñ —Å–∫—Ä–∏–ø—Ç–∏ –∑ 1-2 –∑–∞–ø–∏—Ç–∞–º–∏

# %% [markdown]
#  ---
# 
#  ## 2Ô∏è‚É£ SQL –û—Å–Ω–æ–≤–∏ —Ç–∞ PostgreSQL
# 
# 
# 
#  ### üéØ –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ:
# 
#  - –í–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –±–∞–∑–æ–≤—ñ SQL –∑–∞–ø–∏—Ç–∏ (SELECT, WHERE, ORDER BY)
# 
#  - –û–±'—î–¥–Ω—É–≤–∞—Ç–∏ —Ç–∞–±–ª–∏—Ü—ñ —á–µ—Ä–µ–∑ JOIN
# 
#  - –ê–≥—Ä–µ–≥—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ (GROUP BY, HAVING)
# 
#  - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Window Functions
# 
#  - –ü–∏—Å–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–ø–∏—Ç–∏ –∑ SUBQUERY

# %% [markdown]
#  ### 2.1 –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ë–∞–∑–∏ –î–∞–Ω–∏—Ö
# 
# 
# 
#  –ù–∞—à–∞ –Ω–∞–≤—á–∞–ª—å–Ω–∞ –ë–î –º—ñ—Å—Ç–∏—Ç—å 7 —Ç–∞–±–ª–∏—Ü—å –∑ —Ä–µ–∞–ª—å–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏ e-commerce:
# 
#  - **customers** - –∫–ª—ñ—î–Ω—Ç–∏ (100+ –∑–∞–ø–∏—Å—ñ–≤)
# 
#  - **products** - —Ç–æ–≤–∞—Ä–∏ (50+ –ø–æ–∑–∏—Ü—ñ–π)
# 
#  - **categories** - –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–æ–≤–∞—Ä—ñ–≤
# 
#  - **orders** - –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è
# 
#  - **order_items** - –ø–æ–∑–∏—Ü—ñ—ó –≤ –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è—Ö
# 
#  - **departments** - –≤—ñ–¥–¥—ñ–ª–∏ –∫–æ–º–ø–∞–Ω—ñ—ó
# 
#  - **employees** - –ø—Ä–∞—Ü—ñ–≤–Ω–∏–∫–∏

# %%
# –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –ë–î —Ç–∞ –æ–≥–ª—è–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
if db_ok:
    conn = psycopg2.connect(
        host="localhost",
        database="learning_db",
        user="admin",
        password="admin123"
    )
    cursor = conn.cursor()

    print("=" * 80)
    print("–°–¢–†–£–ö–¢–£–†–ê –ë–ê–ó–ò –î–ê–ù–ò–•")
    print("=" * 80)

    # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ —Ç–∞–±–ª–∏—Ü—å
    cursor.execute("""
        SELECT table_name
        FROM information_schema.tables
        WHERE table_schema = 'public'
        AND table_type = 'BASE TABLE'
        ORDER BY table_name;
    """)

    tables = cursor.fetchall()

    for (table_name,) in tables:
        # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤
        cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
        count = cursor.fetchone()[0]

        # –ö–æ–ª–æ–Ω–∫–∏
        cursor.execute(f"""
            SELECT column_name, data_type
            FROM information_schema.columns
            WHERE table_name = '{table_name}'
            ORDER BY ordinal_position
            LIMIT 5;
        """)
        columns = cursor.fetchall()

        print(f"\nüìã {table_name.upper()}")
        print(f"   –ó–∞–ø–∏—Å—ñ–≤: {count:,}")
        print(f"   –ö–æ–ª–æ–Ω–∫–∏: {', '.join([col[0] for col in columns])}")

    cursor.close()
    conn.close()
else:
    print("‚ö†Ô∏è –ë–∞–∑–∞ –¥–∞–Ω–∏—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –æ–≥–ª—è–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏")


# %% [markdown]
#  ### 2.2 –ë–∞–∑–æ–≤—ñ SQL –ó–∞–ø–∏—Ç–∏
# 
# 
# 
#  –†–æ–∑–≥–ª—è–Ω–µ–º–æ –æ—Å–Ω–æ–≤–Ω—ñ —Ç–∏–ø–∏ –∑–∞–ø–∏—Ç—ñ–≤.

# %%
if db_ok:
    conn = psycopg2.connect(
        host="localhost",
        database="learning_db",
        user="admin",
        password="admin123"
    )
    cursor = conn.cursor(cursor_factory=RealDictCursor)

    print("=" * 80)
    print("–ü–†–ò–ö–õ–ê–î 1: SELECT –∑ WHERE —Ç–∞ ORDER BY")
    print("=" * 80)
    print("–ó–∞–ø–∏—Ç: –¢–æ–ø-5 –∫–ª—ñ—î–Ω—Ç—ñ–≤ –∑ –Ω–∞–π–±—ñ–ª—å—à–∏–º–∏ –≤–∏—Ç—Ä–∞—Ç–∞–º–∏\n")

    query = """
    SELECT
        c.id AS customer_id,
        c.first_name || ' ' || c.last_name AS full_name,
        c.email,
        SUM(oi.quantity * oi.price_at_purchase) AS total_spent
    FROM customers c
    JOIN orders o       ON o.customer_id = c.id
    JOIN order_items oi ON oi.order_id   = o.id
    GROUP BY c.id, c.first_name, c.last_name, c.email
    ORDER BY total_spent DESC
    LIMIT 5;

    """

    cursor.execute(query)
    results = cursor.fetchall()

    for row in results:
        print(f"üë§ {row['full_name']:25} | üí∞ {row['total_spent']:10.2f} –≥—Ä–Ω | üìß {row['email']}")

    cursor.close()
    conn.close()


# %% [markdown]
#  ### 2.3 JOIN - –û–±'—î–¥–Ω–∞–Ω–Ω—è –¢–∞–±–ª–∏—Ü—å
# 
# 
# 
#  JOIN –¥–æ–∑–≤–æ–ª—è—î –∫–æ–º–±—ñ–Ω—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –∑ –∫—ñ–ª—å–∫–æ—Ö —Ç–∞–±–ª–∏—Ü—å.

# %%
if db_ok:
    conn = psycopg2.connect(
        host="localhost",
        database="learning_db",
        user="admin",
        password="admin123"
    )
    cursor = conn.cursor(cursor_factory=RealDictCursor)

    print("=" * 80)
    print("–ü–†–ò–ö–õ–ê–î 2: INNER JOIN - –ü—Ä–æ–¥–∞–∂—ñ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö")
    print("=" * 80)

    query = """
    SELECT
        cat.id AS category_id,
        cat.name AS category_name,
        COUNT(DISTINCT o.id)            AS orders_count,
        SUM(oi.quantity)                AS total_items_sold,
        SUM(oi.quantity * oi.price_at_purchase) AS revenue
    FROM categories AS cat
    INNER JOIN products     AS p  ON p.category_id = cat.id
    INNER JOIN order_items  AS oi ON oi.product_id = p.id
    INNER JOIN orders       AS o  ON o.id = oi.order_id
    GROUP BY cat.id, cat.name
    ORDER BY revenue DESC;

    """

    cursor.execute(query)
    results = cursor.fetchall()

    print(f"\n{'–ö–∞—Ç–µ–≥–æ—Ä—ñ—è':20} | {'–ó–∞–º–æ–≤–ª–µ–Ω—å':>10} | {'–¢–æ–≤–∞—Ä—ñ–≤':>10} | {'–î–æ—Ö—ñ–¥':>15}")
    print("-" * 80)
    for row in results:
        print(f"{row['category_name']:20} | {row['orders_count']:10} | {row['total_items_sold']:10} | {row['revenue']:15.2f} –≥—Ä–Ω")

    cursor.close()
    conn.close()


# %% [markdown]
#  ### 2.4 Window Functions - –ê–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ –§—É–Ω–∫—Ü—ñ—ó
# 
# 
# 
#  Window Functions –¥–æ–∑–≤–æ–ª—è—é—Ç—å —Ä–æ–±–∏—Ç–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è "–ø–æ –≤—ñ–∫–Ω–∞—Ö" –¥–∞–Ω–∏—Ö –±–µ–∑ GROUP BY.

# %%
if db_ok:
    conn = psycopg2.connect(
        host="localhost",
        database="learning_db",
        user="admin",
        password="admin123"
    )
    cursor = conn.cursor(cursor_factory=RealDictCursor)

    print("=" * 80)
    print("–ü–†–ò–ö–õ–ê–î 3: Window Functions - –†–∞–Ω–∂—É–≤–∞–Ω–Ω—è –∫–ª—ñ—î–Ω—Ç—ñ–≤")
    print("=" * 80)

    query = """
    WITH customer_stats AS (
        SELECT
            c.id AS customer_id,
            c.first_name || ' ' || c.last_name AS full_name,
            COUNT(DISTINCT o.id) AS total_orders,
            COALESCE(SUM(oi.quantity * oi.price_at_purchase), 0) AS total_spent
        FROM customers c
        LEFT JOIN orders o       ON o.customer_id = c.id
        LEFT JOIN order_items oi ON oi.order_id   = o.id
        GROUP BY c.id, c.first_name, c.last_name
    )
    SELECT
        full_name,
        total_orders,
        total_spent,
        ROW_NUMBER() OVER (ORDER BY total_spent DESC) AS rank,
        NTILE(4)    OVER (ORDER BY total_spent DESC) AS quartile
    FROM customer_stats
    ORDER BY total_spent DESC
    LIMIT 10;
    """

    cursor.execute(query)
    results = cursor.fetchall()

    print(f"\n{'–ö–ª—ñ—î–Ω—Ç':25} | {'–ó–∞–º–æ–≤–ª–µ–Ω—å':>10} | {'–í–∏—Ç—Ä–∞—á–µ–Ω–æ':>12} | {'–†–∞–Ω–≥':>5} | {'–ö–≤–∞—Ä—Ç–∏–ª—å':>10}")
    print("-" * 90)
    for row in results:
        print(f"{row['full_name']:25} | {row['total_orders']:10} | {row['total_spent']:12.2f} | {row['rank']:5} | {row['quartile']:10}")

    cursor.close()
    conn.close()


# %% [markdown]
#  ### üìä –í–∏—Å–Ω–æ–≤–∫–∏ –∑ SQL –û—Å–Ω–æ–≤
# 
# 
# 
#  **–©–æ –º–∏ –Ω–∞–≤—á–∏–ª–∏—Å—è:**
# 
#  - ‚úÖ SELECT –¥–ª—è –≤–∏–±—ñ—Ä–∫–∏ –¥–∞–Ω–∏—Ö
# 
#  - ‚úÖ WHERE –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
# 
#  - ‚úÖ JOIN –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—å
# 
#  - ‚úÖ GROUP BY –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü—ñ—ó
# 
#  - ‚úÖ Window Functions –¥–ª—è –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏
# 
# 
# 
#  **–¢–∏–ø–∏ JOIN:**
# 
#  - INNER JOIN - —Ç—ñ–ª—å–∫–∏ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è
# 
#  - LEFT JOIN - –≤—Å—ñ –∑–∞–ø–∏—Å–∏ –∑ –ª—ñ–≤–æ—ó —Ç–∞–±–ª–∏—Ü—ñ
# 
#  - RIGHT JOIN - –≤—Å—ñ –∑–∞–ø–∏—Å–∏ –∑ –ø—Ä–∞–≤–æ—ó —Ç–∞–±–ª–∏—Ü—ñ
# 
#  - FULL OUTER JOIN - –≤—Å—ñ –∑–∞–ø–∏—Å–∏ –∑ –æ–±–æ—Ö —Ç–∞–±–ª–∏—Ü—å
# 
# 
# 
#  **Window Functions:**
# 
#  - ROW_NUMBER() - –ø–æ—Ä—è–¥–∫–æ–≤–∏–π –Ω–æ–º–µ—Ä
# 
#  - RANK() - —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è –∑ –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
# 
#  - NTILE(n) - —Ä–æ–∑–ø–æ–¥—ñ–ª –ø–æ –≥—Ä—É–ø–∞—Ö
# 
#  - LAG/LEAD - –¥–æ—Å—Ç—É–ø –¥–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö/–Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤

# %% [markdown]
#  ---
# 
#  ## 3Ô∏è‚É£ Python + PostgreSQL
# 
# 
# 
#  ### üéØ –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ:
# 
#  - –ü—ñ–¥–∫–ª—é—á–∞—Ç–∏—Å—è –¥–æ PostgreSQL —á–µ—Ä–µ–∑ psycopg2
# 
#  - –í–∏–∫–æ–Ω—É–≤–∞—Ç–∏ CRUD –æ–ø–µ—Ä–∞—Ü—ñ—ó
# 
#  - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ parameterized queries –¥–ª—è –∑–∞—Ö–∏—Å—Ç—É –≤—ñ–¥ SQL Injection
# 
#  - –ü—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è–º–∏
# 
#  - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ context managers

# %% [markdown]
#  ### 3.1 –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è —Ç–∞ –ë–∞–∑–æ–≤—ñ –û–ø–µ—Ä–∞—Ü—ñ—ó

# %%
if db_ok:
    from contextlib import contextmanager
    from datetime import datetime
    import psycopg2
    from psycopg2.extras import RealDictCursor

    @contextmanager
    def get_db_connection():
        """Context manager –¥–ª—è –±–µ–∑–ø–µ—á–Ω–æ—ó —Ä–æ–±–æ—Ç–∏ –∑ –ë–î"""
        conn = None
        try:
            conn = psycopg2.connect(
                host="localhost",
                database="learning_db",
                user="admin",
                password="admin123"
            )
            yield conn
            conn.commit()
        except Exception as e:
            if conn:
                conn.rollback()
            raise e
        finally:
            if conn:
                conn.close()

    print("=" * 80)
    print("CRUD –û–ü–ï–†–ê–¶–Ü–á –ó PYTHON")
    print("=" * 80)

    # CREATE - –î–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–æ–≥–æ –∫–ª—ñ—î–Ω—Ç–∞
    print("\n1Ô∏è‚É£ CREATE - –î–æ–¥–∞–≤–∞–Ω–Ω—è –∫–ª—ñ—î–Ω—Ç–∞")
    print("-" * 80)

    with get_db_connection() as conn:
        cursor = conn.cursor()

        query = """
        INSERT INTO customers (first_name, last_name, email, phone, city, country, registration_date)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        RETURNING id;
        """

        new_customer = (
            "–¢–µ—Å—Ç",
            "–¢–µ—Å—Ç–æ–≤–∏—á",
            f"test_{datetime.now().timestamp()}@example.com",
            "+380991234567",
            "Kyiv",          # <‚Äî –¥–æ–¥–∞–Ω–æ city
            "Ukraine",
            datetime.now().date()
        )

        cursor.execute(query, new_customer)
        customer_id = cursor.fetchone()[0]

        print(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ –∫–ª—ñ—î–Ω—Ç–∞ –∑ ID: {customer_id}")
        cursor.close()

    # READ - –ß–∏—Ç–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö
    print("\n2Ô∏è‚É£ READ - –ß–∏—Ç–∞–Ω–Ω—è –∫–ª—ñ—î–Ω—Ç–∞")
    print("-" * 80)

    with get_db_connection() as conn:
        cursor = conn.cursor(cursor_factory=RealDictCursor)

        query = "SELECT * FROM customers WHERE id = %s;"
        cursor.execute(query, (customer_id,))

        customer = cursor.fetchone()
        print(f"üë§ {customer['first_name']} {customer['last_name']}")
        print(f"   Email: {customer['email']}")
        print(f"   Phone: {customer['phone']}")

        cursor.close()

    # UPDATE - –û–Ω–æ–≤–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    print("\n3Ô∏è‚É£ UPDATE - –û–Ω–æ–≤–ª–µ–Ω–Ω—è email")
    print("-" * 80)

    with get_db_connection() as conn:
        cursor = conn.cursor()

        new_email = f"updated_{datetime.now().timestamp()}@example.com"
        query = "UPDATE customers SET email = %s WHERE id = %s;"

        cursor.execute(query, (new_email, customer_id))
        print(f"‚úÖ Email –æ–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞: {new_email}")

        cursor.close()

    # DELETE - –í–∏–¥–∞–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    print("\n4Ô∏è‚É£ DELETE - –í–∏–¥–∞–ª–µ–Ω–Ω—è –∫–ª—ñ—î–Ω—Ç–∞")
    print("-" * 80)

    with get_db_connection() as conn:
        cursor = conn.cursor()

        query = "DELETE FROM customers WHERE id = %s;"
        cursor.execute(query, (customer_id,))

        print(f"‚úÖ –ö–ª—ñ—î–Ω—Ç –∑ ID {customer_id} –≤–∏–¥–∞–ª–µ–Ω–æ")
        cursor.close()


# %% [markdown]
#  ### 3.2 –ó–∞—Ö–∏—Å—Ç –≤—ñ–¥ SQL Injection
# 
# 
# 
#  **–ù–Ü–ö–û–õ–ò –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ string formatting –¥–ª—è SQL –∑–∞–ø–∏—Ç—ñ–≤!**

# %%
print("=" * 80)
print("SQL INJECTION - –ù–µ–±–µ–∑–ø–µ—á–Ω—ñ vs –ë–µ–∑–ø–µ—á–Ω—ñ –ø—Ä–∞–∫—Ç–∏–∫–∏")
print("=" * 80)

# ‚ùå –ù–ï–ë–ï–ó–ü–ï–ß–ù–û - –≤—Ä–∞–∑–ª–∏–≤–æ –¥–æ SQL Injection
print("\n‚ùå –ù–ï–ë–ï–ó–ü–ï–ß–ù–û (–ù–ï –†–û–ë–Ü–¢–¨ –¢–ê–ö!):")
print("-" * 80)
user_input = "test@example.com"
dangerous_query = f"SELECT * FROM customers WHERE email = '{user_input}';"
print(f"Query: {dangerous_query}")
print("‚ö†Ô∏è –Ø–∫—â–æ user_input = \"' OR '1'='1\", –æ—Ç—Ä–∏–º–∞—î–º–æ –≤—Å—ñ –∑–∞–ø–∏—Å–∏!")

# ‚úÖ –ë–ï–ó–ü–ï–ß–ù–û - parameterized query
print("\n‚úÖ –ë–ï–ó–ü–ï–ß–ù–û (–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –∑–∞–≤–∂–¥–∏!):")
print("-" * 80)
safe_query = "SELECT * FROM customers WHERE email = %s;"
print(f"Query: {safe_query}")
print(f"Params: ('{user_input}',)")
print("‚úÖ psycopg2 –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –µ–∫—Ä–∞–Ω—É—î —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Å–∏–º–≤–æ–ª–∏")


# %% [markdown]
#  ### 3.3 –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó
# 
# 
# 
#  –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó –¥–æ–∑–≤–æ–ª—è—é—Ç—å –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –∫—ñ–ª—å–∫–∞ –æ–ø–µ—Ä–∞—Ü—ñ–π –∞—Ç–æ–º–∞—Ä–Ω–æ (–≤—Å–µ –∞–±–æ –Ω—ñ—á–æ–≥–æ).

# %%
if db_ok:
    print("=" * 80)
    print("–ü–†–ò–ö–õ–ê–î –¢–†–ê–ù–ó–ê–ö–¶–Ü–á - –ü–µ—Ä–µ–∫–∞–∑ –º—ñ–∂ —Ä–∞—Ö—É–Ω–∫–∞–º–∏")
    print("=" * 80)

    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()

            # –°–∏–º—É–ª—è—Ü—ñ—è –ø–µ—Ä–µ–∫–∞–∑—É –≥—Ä–æ—à–µ–π
            sender_id = 1
            receiver_id = 2
            amount = 100.0

            print(f"\nüí∏ –ü–µ—Ä–µ–∫–∞–∑ {amount} –≥—Ä–Ω –≤—ñ–¥ –∫–ª—ñ—î–Ω—Ç–∞ {sender_id} –¥–æ {receiver_id}")
            print("-" * 80)

            # –Ø–∫—â–æ —Ç—É—Ç –≤–∏–Ω–∏–∫–Ω–µ –ø–æ–º–∏–ª–∫–∞, –≤—Å—ñ –∑–º—ñ–Ω–∏ –±—É–¥—É—Ç—å —Å–∫–∞—Å–æ–≤–∞–Ω—ñ
            print("1. –°–ø–∏—Å–∞–Ω–Ω—è –∑ –≤—ñ–¥–ø—Ä–∞–≤–Ω–∏–∫–∞...")
            print("2. –ó–∞—Ä–∞—Ö—É–≤–∞–Ω–Ω—è –æ—Ç—Ä–∏–º—É–≤–∞—á—É...")
            print("3. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–∞–ø–∏—Å—É –ø—Ä–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—é...")

            # –Ø–∫—â–æ –≤—Å–µ –û–ö, –∑–º—ñ–Ω–∏ –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ (commit –≤–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)
            print("\n‚úÖ –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞ - –≤—Å—ñ –∑–º—ñ–Ω–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ")

            cursor.close()

    except Exception as e:
        print(f"\n‚ùå –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è —Å–∫–∞—Å–æ–≤–∞–Ω–∞ - rollback –≤–∏–∫–æ–Ω–∞–Ω–æ")
        print(f"   –ü–æ–º–∏–ª–∫–∞: {e}")


# %% [markdown]
#  ### üìä –í–∏—Å–Ω–æ–≤–∫–∏ –∑ Python + PostgreSQL
# 
# 
# 
#  **–©–æ –º–∏ –Ω–∞–≤—á–∏–ª–∏—Å—è:**
# 
#  - ‚úÖ psycopg2 –¥–ª—è –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ PostgreSQL
# 
#  - ‚úÖ CRUD –æ–ø–µ—Ä–∞—Ü—ñ—ó (Create, Read, Update, Delete)
# 
#  - ‚úÖ Parameterized queries –¥–ª—è –±–µ–∑–ø–µ–∫–∏
# 
#  - ‚úÖ Context managers –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ commit/rollback
# 
#  - ‚úÖ –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó –¥–ª—è –∞—Ç–æ–º–∞—Ä–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π
# 
# 
# 
#  **Best Practices:**
# 
#  - ‚úÖ –ó–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ parameterized queries
# 
#  - ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ context managers
# 
#  - ‚úÖ –û–±—Ä–æ–±–ª—è–π—Ç–µ –≤–∏–Ω—è—Ç–∫–∏
# 
#  - ‚úÖ –ó–∞–∫—Ä–∏–≤–∞–π—Ç–µ –∑'—î–¥–Ω–∞–Ω–Ω—è
# 
#  - ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ connection pooling –¥–ª—è production

# %% [markdown]
#  ---
# 
#  ## 4Ô∏è‚É£ –ê–Ω–∞–ª—ñ–∑ –î–∞–Ω–∏—Ö –∑ pandas
# 
# 
# 
#  ### üéØ –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ:
# 
#  - –Ü–Ω—Ç–µ–≥—Ä—É–≤–∞—Ç–∏ pandas –∑ PostgreSQL
# 
#  - –í–∏–∫–æ–Ω—É–≤–∞—Ç–∏ RFM –∞–Ω–∞–ª—ñ–∑
# 
#  - –í—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
# 
#  - –°—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –∑–≤—ñ—Ç–∏

# %% [markdown]
#  ### 4.1 pandas + PostgreSQL –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è

# %%
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–∏–ª—é –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

if db_ok:
    print("=" * 80)
    print("PANDAS + POSTGRESQL")
    print("=" * 80)

    conn = psycopg2.connect(
        host="localhost",
        database="learning_db",
        user="admin",
        password="admin123"
    )

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –≤ DataFrame
    query = """
    SELECT
        c.id AS customer_id,
        c.first_name || ' ' || c.last_name AS customer_name,
        c.country,
        COUNT(DISTINCT o.id) AS total_orders,
        SUM(oi.quantity * oi.price_at_purchase) AS total_spent,
        MAX(o.order_date) AS last_order_date
    FROM customers c
    LEFT JOIN orders o       ON o.customer_id = c.id
    LEFT JOIN order_items oi ON oi.order_id   = o.id
    GROUP BY c.id, c.first_name, c.last_name, c.country
    HAVING COUNT(DISTINCT o.id) > 0;

    """

    df = pd.read_sql_query(query, conn)
    conn.close()

    print(f"\nüìä –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∫–ª—ñ—î–Ω—Ç—ñ–≤")
    print("\n–ü–µ—Ä—à—ñ 5 –∑–∞–ø–∏—Å—ñ–≤:")
    display(df.head())

    print("\nüìà –ë–∞–∑–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    display(df[['total_orders', 'total_spent']].describe())


# %%
df

# %% [markdown]
#  ### 4.2 RFM –ê–Ω–∞–ª—ñ–∑
# 
# 
# 
#  **RFM (Recency, Frequency, Monetary)** - –º–µ—Ç–æ–¥ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó –∫–ª—ñ—î–Ω—Ç—ñ–≤:
# 
#  - **Recency** - —è–∫ –¥–∞–≤–Ω–æ –±—É–≤ –æ—Å—Ç–∞–Ω–Ω—ñ–π –∑–∞–∫–∞–∑
# 
#  - **Frequency** - —è–∫ —á–∞—Å—Ç–æ —Ä–æ–±–∏—Ç—å –∑–∞–∫–∞–∑–∏
# 
#  - **Monetary** - —Å–∫—ñ–ª—å–∫–∏ –≤–∏—Ç—Ä–∞—á–∞—î –≥—Ä–æ—à–µ–π

# %%
import numpy as np

def quantile_score(s: pd.Series, n: int = 5, ascending: bool = True) -> pd.Series:
    """
    –î–∞—î –æ—Ü—ñ–Ω–∫—É 1..n –∑–∞ –∫–≤–∞–Ω—Ç–∏–ª—è–º–∏. –ü—Ä–∞—Ü—é—î —Å—Ç–∞–±—ñ–ª—å–Ω–æ –Ω–∞–≤—ñ—Ç—å –∫–æ–ª–∏ –±–∞–≥–∞—Ç–æ –æ–¥–Ω–∞–∫–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å.
    - ascending=True: –±—ñ–ª—å—à–µ –∑–Ω–∞—á–µ–Ω–Ω—è ‚Üí –≤–∏—â–∏–π –±–∞–ª (–¥–ª—è F —Ç–∞ M)
    - ascending=False: –º–µ–Ω—à–µ –∑–Ω–∞—á–µ–Ω–Ω—è ‚Üí –≤–∏—â–∏–π –±–∞–ª (–¥–ª—è R: –º–µ–Ω—à–∞ –¥–∞–≤–Ω—ñ—Å—Ç—å = –∫—Ä–∞—â–µ)
    """
    s = s.copy()
    mask = s.notna()
    k = int(min(n, mask.sum()))  # —Å–∫—ñ–ª—å–∫–∏ —Ä–µ–∞–ª—å–Ω–æ –º–æ–∂–Ω–∞ –∑—Ä–æ–±–∏—Ç–∏ –±—ñ–Ω—ñ–≤

    if k == 0:
        return pd.Series(np.nan, index=s.index)

    # –†–æ–±–∏–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ —Ä–∞–Ω–≥–∏, —â–æ–± –Ω–µ –±—É–ª–æ –¥—É–±–ª—ñ–≤ –Ω–∞ –º–µ–∂–∞—Ö
    ranks = s[mask].rank(method="first", ascending=ascending)

    # –ü–æ–¥—ñ–ª—è—î–º–æ —Ä–∞–Ω–≥–∏ –Ω–∞ k –∫–≤–∞–Ω—Ç–∏–ª—ñ–≤ —ñ –¥–∞—î–º–æ –º—ñ—Ç–∫–∏ 1..k
    bins = pd.qcut(ranks, q=k, labels=list(range(1, k + 1)))

    out = pd.Series(np.nan, index=s.index, dtype="float")
    out.loc[mask] = bins.astype(int)
    return out

if db_ok and 'df' in locals():
    print("=" * 80)
    print("RFM –°–ï–ì–ú–ï–ù–¢–ê–¶–Ü–Ø –ö–õ–Ü–Ñ–ù–¢–Ü–í")
    print("=" * 80)

    # –û–±—á–∏—Å–ª–µ–Ω–Ω—è RFM –º–µ—Ç—Ä–∏–∫
    today = pd.Timestamp.now()
    df['last_order_date'] = pd.to_datetime(df['last_order_date'])
    df['recency_days'] = (today - df['last_order_date']).dt.days

    # –ù–∞–¥—ñ–π–Ω—ñ RFM-–æ—Ü—ñ–Ω–∫–∏ (1..5)
    df['R_score'] = quantile_score(df['recency_days'], n=5, ascending=False)  # –º–µ–Ω—à–µ –¥–Ω—ñ–≤ = –∫—Ä–∞—â–µ
    df['F_score'] = quantile_score(df['total_orders'], n=5, ascending=True)   # –±—ñ–ª—å—à–µ –∑–∞–º–æ–≤–ª–µ–Ω—å = –∫—Ä–∞—â–µ
    df['M_score'] = quantile_score(df['total_spent'], n=5, ascending=True)    # –±—ñ–ª—å—à–µ –≤–∏—Ç—Ä–∞—Ç = –∫—Ä–∞—â–µ

    # –ó–∞–≥–∞–ª—å–Ω–∏–π RFM score (—è–∫ —Ä—è–¥–æ–∫, –Ω–∞–ø—Ä. "543")
    df['RFM_score'] = (
        df['R_score'].astype('Int64').astype(str) +
        df['F_score'].astype('Int64').astype(str) +
        df['M_score'].astype('Int64').astype(str)
    )

    # –°–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è (–∑–∞—Ö–∏—Å—Ç –≤—ñ–¥ NaN)
    def segment_customer(row):
        if pd.isna(row['R_score']) or pd.isna(row['F_score']) or pd.isna(row['M_score']):
            return 'Unknown'
        r, f, m = int(row['R_score']), int(row['F_score']), int(row['M_score'])
        if r >= 4 and f >= 4 and m >= 4:
            return 'VIP'
        elif r >= 4 and f >= 3:
            return 'Loyal'
        elif r >= 4:
            return 'Promising'
        elif f >= 4:
            return 'At Risk'
        else:
            return 'Lost'

    df['segment'] = df.apply(segment_customer, axis=1)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
    print("\nüìä –†–æ–∑–ø–æ–¥—ñ–ª –∫–ª—ñ—î–Ω—Ç—ñ–≤ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö:")
    print(df['segment'].value_counts(dropna=False))

    print("\nüë• –ü—Ä–∏–∫–ª–∞–¥–∏ –∫–ª—ñ—î–Ω—Ç—ñ–≤ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö:")
    for segment, grp in df.groupby('segment'):
        sample = grp.iloc[0]
        print(f"\n{segment}:")
        print(f"  üë§ {sample['customer_name']}")
        print(f"  üìÖ –û—Å—Ç–∞–Ω–Ω—î –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è: {int(sample['recency_days'])} –¥–Ω—ñ–≤ —Ç–æ–º—É")
        print(f"  üõí –í—Å—å–æ–≥–æ –∑–∞–º–æ–≤–ª–µ–Ω—å: {int(sample['total_orders'])}")
        print(f"  üí∞ –í—Å—å–æ–≥–æ –≤–∏—Ç—Ä–∞—á–µ–Ω–æ: {float(sample['total_spent']):.2f} –≥—Ä–Ω")


# %% [markdown]
#  ### 4.3 –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –†–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤

# %%
if db_ok and 'df' in locals():
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.ticker import FuncFormatter, MaxNLocator

    print("=" * 80)
    print("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –ê–ù–ê–õ–Ü–ó–£ (v2)")
    print("=" * 80)

    # ---- –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ ----
    segment_order = ['VIP', 'Loyal', 'At Risk', 'Lost', 'Unknown']
    df['segment'] = pd.Categorical(df['segment'], categories=segment_order, ordered=True)

    # —Ñ–æ—Ä–º–∞—Ç –≥—Ä–∏–≤–Ω—ñ –∑ –ø—Ä–æ–±—ñ–ª–æ–º: 123 456
    fmt_uah = FuncFormatter(lambda x, pos: f"{int(x):,}".replace(",", " "))

    fig, axes = plt.subplots(2, 2, figsize=(18, 10), constrained_layout=True)

    # 1) –î–æ–Ω–∞—Ç: —Ä–æ–∑–ø–æ–¥—ñ–ª –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö + —Ü–µ–Ω—Ç—Ä –∑ total
    seg_counts = df['segment'].value_counts().reindex(segment_order).dropna()
    wedges, texts, autotexts = axes[0, 0].pie(
        seg_counts.values,
        labels=seg_counts.index.astype(str),
        autopct=lambda p: f"{p:.1f}%\n({int(round(p/100*seg_counts.sum()))})",
        startangle=90,
        pctdistance=0.8
    )
    centre = plt.Circle((0, 0), 0.56, fc='white')
    axes[0, 0].add_artist(centre)
    axes[0, 0].axis('equal')
    axes[0, 0].set_title('–†–æ–∑–ø–æ–¥—ñ–ª –∫–ª—ñ—î–Ω—Ç—ñ–≤ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö')
    axes[0, 0].text(0, 0.02, f"–ö–ª—ñ—î–Ω—Ç—ñ–≤\n{int(seg_counts.sum())}", ha='center', va='center', fontsize=11)

    # 2) –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∏–π –±–∞—Ä—á–∞—Ä—Ç: –≤–∏—Ç—Ä–∞—Ç–∏ + —á–∞—Å—Ç–∫–∞ %
    revenue_by_segment = (
        df.groupby('segment', observed=True)['total_spent']
          .sum()
          .reindex(segment_order)
          .dropna()
          .sort_values()
    )
    total_rev = revenue_by_segment.sum()
    axes[0, 1].barh(revenue_by_segment.index.astype(str), revenue_by_segment.values)
    axes[0, 1].xaxis.set_major_formatter(fmt_uah)
    axes[0, 1].grid(axis='x', linestyle='--', alpha=0.35)
    for i, v in enumerate(revenue_by_segment.values):
        share = v / total_rev * 100 if total_rev else 0
        axes[0, 1].text(v, i, f" {int(v):,}".replace(",", " ") + f"  ({share:.1f}%)", va='center')
    axes[0, 1].set_title('–ó–∞–≥–∞–ª—å–Ω—ñ –≤–∏—Ç—Ä–∞—Ç–∏ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö')
    axes[0, 1].set_xlabel('–í–∏—Ç—Ä–∞—Ç–∏ (–≥—Ä–Ω)')

    # 3) –ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∞ –≤–∏—Ç—Ä–∞—Ç + –∫–≤–∞—Ä—Ç–∏–ª—ñ + CDF
    spent = df['total_spent'].dropna()
    if len(spent) > 1:
        bins = np.histogram_bin_edges(spent, bins='fd')
    else:
        bins = 5
    axes[1, 0].hist(spent, bins=bins, edgecolor='black')
    q25, q50, q75 = np.percentile(spent, [25, 50, 75])
    axes[1, 0].axvline(q50, linestyle='--', linewidth=1, label=f"–ú–µ–¥—ñ–∞–Ω–∞: {q50:.0f}")
    axes[1, 0].axvspan(q25, q75, alpha=0.12, label='IQR (Q1‚ÄìQ3)')
    axes[1, 0].xaxis.set_major_formatter(fmt_uah)
    axes[1, 0].grid(axis='y', linestyle='--', alpha=0.35)
    axes[1, 0].set_title('–†–æ–∑–ø–æ–¥—ñ–ª –≤–∏—Ç—Ä–∞—Ç –∫–ª—ñ—î–Ω—Ç—ñ–≤')
    axes[1, 0].set_xlabel('–í–∏—Ç—Ä–∞—Ç–∏ (–≥—Ä–Ω)')
    axes[1, 0].set_ylabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—ñ—î–Ω—Ç—ñ–≤')
    axes[1, 0].legend()

    # CDF (–≤—Ç–æ—Ä–∏–Ω–Ω–∞ –≤—ñ—Å—å)
    ax_cdf = axes[1, 0].twinx()
    xs = np.sort(spent.values)
    ys = np.arange(1, len(xs) + 1) / len(xs)
    ax_cdf.plot(xs, ys, linewidth=1)
    ax_cdf.set_ylim(0, 1)
    ax_cdf.set_yticks([0, 0.25, 0.5, 0.75, 1])
    ax_cdf.set_ylabel('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞ —á–∞—Å—Ç–∫–∞')

    # 4) –°–∫–∞—Ç–µ—Ä (–±–∞–±–ª-—á–∞—Ä—Ç) –∑ —Ç—Ä–µ–Ω–¥-–ª—ñ–Ω—ñ—î—é —Ç–∞ –ª–µ–≥–∫–∏–º –¥–∂–∏—Ç–µ—Ä–æ–º
    ax = axes[1, 1]
    rng = np.random.default_rng(42)
    for seg, g in df.groupby('segment', observed=True):
        if g.empty:
            continue
        jitter_x = g['total_orders'] + rng.normal(0, 0.03, len(g))  # —â–æ–± —Ç–æ—á–∫–∏ –∑ –æ–¥–Ω–∞–∫–æ–≤–∏–º x –Ω–µ –ø–µ—Ä–µ–∫—Ä–∏–≤–∞–ª–∏—Å—å
        size = ((g['total_spent'] / max(df['total_spent'].max(), 1)) * 500) + 60
        ax.scatter(jitter_x, g['total_spent'], s=size, alpha=0.7, label=str(seg))

    # —Ç—Ä–µ–Ω–¥ –ø–æ –≤—Å—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ (–∑–≤–∏—á–∞–π–Ω–∞ –ª—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è)
    if df['total_orders'].nunique() > 1:
        x = df['total_orders'].values.astype(float)
        y = df['total_spent'].values.astype(float)
        k, b = np.polyfit(x, y, 1)
        xr = np.linspace(x.min(), x.max(), 100)
        ax.plot(xr, k * xr + b, linewidth=1)

    ax.xaxis.set_major_locator(MaxNLocator(integer=True))
    ax.yaxis.set_major_formatter(fmt_uah)
    ax.grid(True, linestyle='--', alpha=0.35)
    ax.set_title('–ß–∞—Å—Ç–æ—Ç–∞ –∑–∞–º–æ–≤–ª–µ–Ω—å vs –í–∏—Ç—Ä–∞—Ç–∏ (—Ä–æ–∑–º—ñ—Ä = –≤–∏—Ç—Ä–∞—Ç–∏)')
    ax.set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–º–æ–≤–ª–µ–Ω—å')
    ax.set_ylabel('–í–∏—Ç—Ä–∞—Ç–∏ (–≥—Ä–Ω)')
    leg = ax.legend(title='–°–µ–≥–º–µ–Ω—Ç', frameon=False, loc='lower right')
    for txt in leg.get_texts():
        txt.set_ha('right')

    # –ü—ñ–¥–ø–∏—Å—É—î–º–æ —Ç–æ–ø-5 –∫–ª—ñ—î–Ω—Ç—ñ–≤ –∑–∞ –≤–∏—Ç—Ä–∞—Ç–∞–º–∏ (–∞–∫—É—Ä–∞—Ç–Ω—ñ –∑—Å—É–≤–∏)
    topN = df.nlargest(5, 'total_spent')
    for i, (_, r) in enumerate(topN.iterrows()):
        ax.annotate(
            r['customer_name'],
            (r['total_orders'], r['total_spent']),
            xytext=(8, 6 + i*2),
            textcoords='offset points',
            fontsize=8
        )

    plt.show()
    print("\n‚úÖ –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—é –ø–æ–∫—Ä–∞—â–µ–Ω–æ")



# %% [markdown]
#  ### üìä –í–∏—Å–Ω–æ–≤–∫–∏ –∑ –ê–Ω–∞–ª—ñ–∑—É –î–∞–Ω–∏—Ö
# 
# 
# 
#  **–©–æ –º–∏ –Ω–∞–≤—á–∏–ª–∏—Å—è:**
# 
#  - ‚úÖ –Ü–Ω—Ç–µ–≥—Ä—É–≤–∞—Ç–∏ pandas –∑ PostgreSQL
# 
#  - ‚úÖ –í–∏–∫–æ–Ω—É–≤–∞—Ç–∏ RFM –∞–Ω–∞–ª—ñ–∑ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó –∫–ª—ñ—î–Ω—Ç—ñ–≤
# 
#  - ‚úÖ –°—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –∑ matplotlib/seaborn
# 
#  - ‚úÖ –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑—É
# 
# 
# 
#  **RFM –°–µ–≥–º–µ–Ω—Ç–∏:**
# 
#  - **VIP** - –Ω–∞–π–∫—Ä–∞—â—ñ –∫–ª—ñ—î–Ω—Ç–∏ (R‚Üë F‚Üë M‚Üë)
# 
#  - **Loyal** - –ª–æ—è–ª—å–Ω—ñ –∫–ª—ñ—î–Ω—Ç–∏ (F‚Üë)
# 
#  - **Promising** - –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ñ (R‚Üë)
# 
#  - **At Risk** - —Ä–∏–∑–∏–∫ –≤—Ç—Ä–∞—Ç–∏ (R‚Üì F‚Üë)
# 
#  - **Lost** - –≤—Ç—Ä–∞—á–µ–Ω—ñ –∫–ª—ñ—î–Ω—Ç–∏ (R‚Üì)

# %% [markdown]
#  ---
# 
#  # –ß–∞—Å—Ç–∏–Ω–∞ 2: Advanced –ú–æ–¥—É–ª—ñ
# 
#  ---
# 
# 
# 
#  –ù–∞—Å—Ç—É–ø–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞ –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è **Senior Data Scientists/Engineers** —Ç–∞ –ø–æ–∫—Ä–∏–≤–∞—î:
# 
#  - Production ETL pipelines
# 
#  - Architectural patterns (Repository, DI, Factory)
# 
#  - ML infrastructure (Feature Store)
# 
#  - Advanced SQL analytics

# %% [markdown]
#  ## 5Ô∏è‚É£ Production ETL Pipeline
# 
# 
# 
#  ### üéØ –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ:
# 
#  - –°—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ async ETL pipelines
# 
#  - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ type safety –∑ @dataclass
# 
#  - –í–∞–ª—ñ–¥—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ
# 
#  - –ó–±–∏—Ä–∞—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏
# 
#  - –û–±—Ä–æ–±–ª—è—Ç–∏ –ø–æ–º–∏–ª–∫–∏ gracefully

# %% [markdown]
#  ### 5.1 –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ ETL Pipeline
# 
# 
# 
#  **ETL** = Extract ‚Üí Transform ‚Üí Load
# 
# 
# 
#  –ù–∞—à pipeline –±—É–¥–µ:
# 
#  - **Extract** - –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≤–∏—Ç—è–≥—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –∑ APIs
# 
#  - **Transform** - –≤–∞–ª—ñ–¥—É–≤–∞—Ç–∏ —Ç–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º—É–≤–∞—Ç–∏
# 
#  - **Load** - –±–∞—Ç—á–∞–º–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ –≤ PostgreSQL
# 
#  - **Monitor** - –∑–±–∏—Ä–∞—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ

# %%
# –Ü–º–ø–æ—Ä—Ç—É—î–º–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∑ advanced_examples/etl/01_async_etl_pipeline.py
from dataclasses import dataclass, field
from typing import List, Dict, Optional
from collections import defaultdict

@dataclass
class WeatherData:
    """–ú–æ–¥–µ–ª—å –¥–∞–Ω–∏—Ö –ø–æ–≥–æ–¥–∏ –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é"""
    city: str
    temperature: float
    humidity: int
    description: str
    timestamp: datetime = field(default_factory=datetime.now)

    def validate(self) -> bool:
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö"""
        if not (-50 <= self.temperature <= 60):
            return False
        if not (0 <= self.humidity <= 100):
            return False
        if not self.city or not self.description:
            return False
        return True

class MetricsCollector:
    """–ó–±—ñ—Ä –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ"""
    def __init__(self):
        self.metrics: Dict[str, int] = defaultdict(int)
        self.start_time = time.time()

    def increment(self, metric: str, value: int = 1):
        self.metrics[metric] += value

    def get_summary(self) -> Dict[str, Any]:
        elapsed = time.time() - self.start_time
        return {
            'elapsed_seconds': round(elapsed, 2),
            'metrics': dict(self.metrics),
            'throughput': round(self.metrics.get('records_processed', 0) / elapsed, 2) if elapsed > 0 else 0
        }

print("=" * 80)
print("ETL PIPELINE COMPONENTS")
print("=" * 80)
print("‚úÖ WeatherData model - type-safe data structure")
print("‚úÖ MetricsCollector - performance monitoring")
print("\nüí° –í production —Ç–∞–∫–æ–∂ –¥–æ–¥–∞–π—Ç–µ:")
print("   - Retry logic –∑ exponential backoff")
print("   - Circuit breaker –¥–ª—è API calls")
print("   - Dead letter queue –¥–ª—è failed records")
print("   - Prometheus metrics export")


# %% [markdown]
#  ### 5.2 Async Extraction
# 
# 
# 
#  –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ –º–Ω–æ–∂–∏–Ω–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ.

# %%
async def extract_weather_data(city: str, metrics: MetricsCollector) -> Optional[WeatherData]:
    """–°–∏–º—É–ª—è—Ü—ñ—è –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –ø—Ä–æ –ø–æ–≥–æ–¥—É"""
    try:
        # –°–∏–º—É–ª—è—Ü—ñ—è API call
        await asyncio.sleep(0.1)  # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ: await session.get(api_url)

        # –°–∏–º—É–ª—è—Ü—ñ—è –æ—Ç—Ä–∏–º–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        import random
        data = WeatherData(
            city=city,
            temperature=round(random.uniform(-10, 35), 1),
            humidity=random.randint(30, 90),
            description=random.choice(['Clear', 'Cloudy', 'Rainy', 'Sunny'])
        )

        metrics.increment('records_extracted')
        return data

    except Exception as e:
        metrics.increment('extraction_errors')
        print(f"‚ùå Error extracting data for {city}: {e}")
        return None

async def extract_all_weather(cities: List[str]) -> List[WeatherData]:
    """–ü–∞—Ä–∞–ª–µ–ª—å–Ω–µ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–ª—è –≤—Å—ñ—Ö –º—ñ—Å—Ç"""
    metrics = MetricsCollector()

    print("=" * 80)
    print("EXTRACTION PHASE")
    print("=" * 80)
    print(f"üåç Extracting weather data for {len(cities)} cities...\n")

    # –ü–∞—Ä–∞–ª–µ–ª—å–Ω–µ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è
    tasks = [extract_weather_data(city, metrics) for city in cities]
    results = await asyncio.gather(*tasks)

    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ None –∑–Ω–∞—á–µ–Ω–Ω—è
    valid_results = [r for r in results if r is not None]

    summary = metrics.get_summary()
    print(f"\n‚úÖ Extraction complete:")
    print(f"   Records extracted: {summary['metrics']['records_extracted']}")
    print(f"   Errors: {summary['metrics'].get('extraction_errors', 0)}")
    print(f"   Duration: {summary['elapsed_seconds']}s")
    print(f"   Throughput: {summary['throughput']} records/sec")

    return valid_results

# –¢–µ—Å—Ç
cities = ['Kyiv', 'Lviv', 'Odesa', 'Kharkiv', 'Dnipro']
weather_data = await extract_all_weather(cities)

print(f"\nüìä Sample extracted data:")
for data in weather_data[:3]:
    print(f"   {data.city}: {data.temperature}¬∞C, {data.humidity}% humidity, {data.description}")


# %% [markdown]
#  ### 5.3 Transformation & Validation

# %%
def transform_weather_data(raw_data: List[WeatherData]) -> tuple[List[WeatherData], List[WeatherData]]:
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö"""
    print("=" * 80)
    print("TRANSFORMATION PHASE")
    print("=" * 80)

    valid_records = []
    invalid_records = []

    for record in raw_data:
        if record.validate():
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó
            # –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –æ–¥–∏–Ω–∏—Ü—å, —Å—Ç–∞–Ω–¥–∞—Ä–¥–∏–∑–∞—Ü—ñ—è —Ñ–æ—Ä–º–∞—Ç—ñ–≤
            valid_records.append(record)
        else:
            invalid_records.append(record)

    print(f"‚úÖ Valid records: {len(valid_records)}")
    print(f"‚ùå Invalid records: {len(invalid_records)}")

    if invalid_records:
        print("\n‚ö†Ô∏è Invalid records:")
        for rec in invalid_records:
            print(f"   {rec}")

    return valid_records, invalid_records

valid_data, invalid_data = transform_weather_data(weather_data)


# %% [markdown]
#  ### 5.4 Batch Loading
# 
# 
# 
#  –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –±–∞—Ç—á–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ.

# %%
if db_ok:
    def load_weather_data_batch(data: List[WeatherData], batch_size: int = 100):
        """–ë–∞—Ç—á–µ–≤–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ UPSERT"""
        print("=" * 80)
        print("LOADING PHASE")
        print("=" * 80)

        with get_db_connection() as conn:
            cursor = conn.cursor()

            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤—É —Ç–∞–±–ª–∏—Ü—é —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS weather_data (
                    id SERIAL PRIMARY KEY,
                    city VARCHAR(100),
                    temperature DECIMAL(5,2),
                    humidity INTEGER,
                    description VARCHAR(200),
                    timestamp TIMESTAMP,
                    UNIQUE(city, timestamp)
                );
            """)

            # –ë–∞—Ç—á–µ–≤–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
            total_loaded = 0
            for i in range(0, len(data), batch_size):
                batch = data[i:i + batch_size]

                # UPSERT (INSERT ... ON CONFLICT)
                query = """
                INSERT INTO weather_data (city, temperature, humidity, description, timestamp)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (city, timestamp) DO UPDATE SET
                    temperature = EXCLUDED.temperature,
                    humidity = EXCLUDED.humidity,
                    description = EXCLUDED.description;
                """

                values = [
                    (r.city, r.temperature, r.humidity, r.description, r.timestamp)
                    for r in batch
                ]

                cursor.executemany(query, values)
                total_loaded += len(batch)
                print(f"üì¶ Loaded batch {i//batch_size + 1}: {len(batch)} records")

            cursor.close()

        print(f"\n‚úÖ Total records loaded: {total_loaded}")
        return total_loaded

    load_weather_data_batch(valid_data)
else:
    print("‚ö†Ô∏è Skipping load phase (DB not available)")


# %% [markdown]
#  ### üìä –í–∏—Å–Ω–æ–≤–∫–∏ –∑ ETL Pipeline
# 
# 
# 
#  **Production Best Practices:**
# 
#  - ‚úÖ **Type Safety** - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ @dataclass –¥–ª—è –º–æ–¥–µ–ª–µ–π –¥–∞–Ω–∏—Ö
# 
#  - ‚úÖ **Validation** - –≤–∞–ª—ñ–¥—É–π—Ç–µ –¥–∞–Ω—ñ –ø–µ—Ä–µ–¥ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è–º
# 
#  - ‚úÖ **Async** - –≤–∏—Ç—è–≥—É–π—Ç–µ –¥–∞–Ω—ñ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
# 
#  - ‚úÖ **Batch Loading** - –∑–∞–≤–∞–Ω—Ç–∞–∂—É–π—Ç–µ –±–∞—Ç—á–∞–º–∏ –¥–ª—è performance
# 
#  - ‚úÖ **Metrics** - –∑–±–∏—Ä–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
# 
#  - ‚úÖ **Error Handling** - graceful failures, retry logic
# 
#  - ‚úÖ **UPSERT** - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ ON CONFLICT –¥–ª—è idempotency
# 
# 
# 
#  **–©–æ –¥–æ–¥–∞—Ç–∏ –¥–∞–ª—ñ:**
# 
#  - Circuit Breaker pattern –¥–ª—è API calls
# 
#  - Dead Letter Queue –¥–ª—è failed records
# 
#  - Prometheus metrics export
# 
#  - Data quality checks (Great Expectations)

# %% [markdown]
#  ---
# 
#  ## 6Ô∏è‚É£ Architectural Patterns
# 
# 
# 
#  ### üéØ –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ:
# 
#  - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Repository Pattern
# 
#  - –ó–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞—Ç–∏ Dependency Injection
# 
#  - –Ü–º–ø–ª–µ–º–µ–Ω—Ç—É–≤–∞—Ç–∏ Factory Pattern
# 
#  - –ü–∏—Å–∞—Ç–∏ testable code

# %% [markdown]
#  ### 6.1 Repository Pattern
# 
# 
# 
#  **Repository Pattern** - –∞–±—Å—Ç—Ä–∞–∫—Ü—ñ—è –¥–ª—è –¥–æ—Å—Ç—É–ø—É –¥–æ –¥–∞–Ω–∏—Ö.
# 
# 
# 
#  **–ü–µ—Ä–µ–≤–∞–≥–∏:**
# 
#  - –í—ñ–¥–¥—ñ–ª–µ–Ω–Ω—è –±—ñ–∑–Ω–µ—Å –ª–æ–≥—ñ–∫–∏ –≤—ñ–¥ data access
# 
#  - –õ–µ–≥–∫–æ —Ç–µ—Å—Ç—É–≤–∞—Ç–∏ (mock repositories)
# 
#  - –ú–æ–∂–Ω–∞ –º—ñ–Ω—è—Ç–∏ –ë–î –±–µ–∑ –∑–º—ñ–Ω–∏ –±—ñ–∑–Ω–µ—Å –ª–æ–≥—ñ–∫–∏

# %%
from abc import ABC, abstractmethod
from typing import Generic, TypeVar, List, Optional

T = TypeVar('T')

class IRepository(ABC, Generic[T]):
    """–ë–∞–∑–æ–≤–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å—ñ—Ö repositories"""

    @abstractmethod
    def get_by_id(self, id: int) -> Optional[T]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –∑–∞–ø–∏—Å –ø–æ ID"""
        pass

    @abstractmethod
    def get_all(self) -> List[T]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –≤—Å—ñ –∑–∞–ø–∏—Å–∏"""
        pass

    @abstractmethod
    def create(self, entity: T) -> T:
        """–°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤–∏–π –∑–∞–ø–∏—Å"""
        pass

    @abstractmethod
    def update(self, entity: T) -> bool:
        """–û–Ω–æ–≤–∏—Ç–∏ –∑–∞–ø–∏—Å"""
        pass

    @abstractmethod
    def delete(self, id: int) -> bool:
        """–í–∏–¥–∞–ª–∏—Ç–∏ –∑–∞–ø–∏—Å"""
        pass

@dataclass
class Customer:
    """Domain model –¥–ª—è –∫–ª—ñ—î–Ω—Ç–∞"""
    customer_id: Optional[int]
    first_name: str
    last_name: str
    email: str
    country: str = "Ukraine"

    @property
    def full_name(self) -> str:
        return f"{self.first_name} {self.last_name}"

class ICustomerRepository(IRepository[Customer]):
    """–°–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è customer repository"""

    @abstractmethod
    def get_by_email(self, email: str) -> Optional[Customer]:
        pass

    @abstractmethod
    def get_by_country(self, country: str) -> List[Customer]:
        pass

print("=" * 80)
print("REPOSITORY PATTERN")
print("=" * 80)
print("‚úÖ IRepository - generic interface")
print("‚úÖ ICustomerRepository - specific interface")
print("‚úÖ Customer - domain model")
print("\nüí° –¢–µ–ø–µ—Ä –º–æ–∂–µ–º–æ —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ —Ä—ñ–∑–Ω—ñ implementations:")
print("   - PostgresCustomerRepository - —Ä–µ–∞–ª—å–Ω–∞ –ë–î")
print("   - InMemoryCustomerRepository - –¥–ª—è —Ç–µ—Å—Ç—ñ–≤")
print("   - MongoCustomerRepository - —è–∫—â–æ –∑–º—ñ–Ω–∏–º–æ –ë–î")


# %% [markdown]
#  ### 6.2 Repository Implementation

# %%
class PostgresCustomerRepository(ICustomerRepository):
    """PostgreSQL implementation of customer repository"""

    # DRY: —Å–ø—ñ–ª—å–Ω–∏–π –º–∞–ø–µ—Ä
    @staticmethod
    def _row_to_customer(row) -> Customer:
        return Customer(
            customer_id=row['id'],          # <- –±—É–ª–æ row['customer_id']
            first_name=row['first_name'],
            last_name=row['last_name'],
            email=row['email'],
            country=row['country']
        )

    def get_by_id(self, id: int) -> Optional[Customer]:
        if not db_ok:
            return None
        with get_db_connection() as conn:
            cursor = conn.cursor(cursor_factory=RealDictCursor)
            cursor.execute("SELECT * FROM customers WHERE id = %s;", (id,))  # <- id
            row = cursor.fetchone()
            cursor.close()
            return self._row_to_customer(row) if row else None

    def get_all(self) -> List[Customer]:
        if not db_ok:
            return []
        with get_db_connection() as conn:
            cursor = conn.cursor(cursor_factory=RealDictCursor)
            cursor.execute("SELECT * FROM customers LIMIT 10;")
            rows = cursor.fetchall()
            cursor.close()
            return [self._row_to_customer(r) for r in rows]

    def get_by_email(self, email: str) -> Optional[Customer]:
        if not db_ok:
            return None
        with get_db_connection() as conn:
            cursor = conn.cursor(cursor_factory=RealDictCursor)
            cursor.execute("SELECT * FROM customers WHERE email = %s;", (email,))
            row = cursor.fetchone()
            cursor.close()
            return self._row_to_customer(row) if row else None

    def get_by_country(self, country: str) -> List[Customer]:
        if not db_ok:
            return []
        with get_db_connection() as conn:
            cursor = conn.cursor(cursor_factory=RealDictCursor)
            cursor.execute("SELECT * FROM customers WHERE country = %s LIMIT 10;", (country,))
            rows = cursor.fetchall()
            cursor.close()
            return [self._row_to_customer(r) for r in rows]

    def create(self, entity: Customer) -> Customer:
        # Implementation...
        pass

    def update(self, entity: Customer) -> bool:
        # Implementation...
        pass

    def delete(self, id: int) -> bool:
        # Implementation...
        pass


# –¢–µ—Å—Ç—É—î–º–æ repository
if db_ok:
    print("=" * 80)
    print("REPOSITORY IN ACTION")
    print("=" * 80)

    repo = PostgresCustomerRepository()

    # Get by ID
    customer = repo.get_by_id(1)
    if customer:
        print(f"‚úÖ Found customer: {customer.full_name} ({customer.email})")

    # Get all
    customers = repo.get_all()
    print(f"‚úÖ Retrieved {len(customers)} customers")

    # Get by country
    ukraine_customers = repo.get_by_country("Ukraine")
    print(f"‚úÖ Found {len(ukraine_customers)} customers from Ukraine")


# %% [markdown]
#  ### 6.3 Dependency Injection & Service Layer

# %%
class CustomerService:
    """Business logic layer –∑ dependency injection"""

    def __init__(self, repository: ICustomerRepository):
        """DI: –æ—Ç—Ä–∏–º—É—î–º–æ repository —á–µ—Ä–µ–∑ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä"""
        self.repository = repository

    def get_customer_summary(self, customer_id: int) -> Optional[Dict[str, Any]]:
        """–ë—ñ–∑–Ω–µ—Å –ª–æ–≥—ñ–∫–∞: –æ—Ç—Ä–∏–º–∞—Ç–∏ summary –∫–ª—ñ—î–Ω—Ç–∞"""
        customer = self.repository.get_by_id(customer_id)

        if not customer:
            return None

        return {
            'id': customer.customer_id,
            'name': customer.full_name,
            'email': customer.email,
            'country': customer.country,
            'risk_score': self._calculate_risk_score(customer)
        }

    def _calculate_risk_score(self, customer: Customer) -> float:
        """–ü—Ä–∏–∫–ª–∞–¥ –±—ñ–∑–Ω–µ—Å –ª–æ–≥—ñ–∫–∏"""
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ —Ç—É—Ç –±—É–ª–∞ –± —Å–∫–ª–∞–¥–Ω–∞ –ª–æ–≥—ñ–∫–∞
        return 0.0

print("=" * 80)
print("DEPENDENCY INJECTION")
print("=" * 80)
print("‚úÖ CustomerService –æ—Ç—Ä–∏–º—É—î repository —á–µ—Ä–µ–∑ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä")
print("‚úÖ –ú–æ–∂–µ–º–æ –ø–µ—Ä–µ–¥–∞—Ç–∏ –±—É–¥—å-—è–∫—É implementation (Postgres, InMemory, Mock)")
print("‚úÖ Service –Ω–µ –∑–Ω–∞—î –ø—Ä–æ –¥–µ—Ç–∞–ª—ñ –ë–î - —Ç—ñ–ª—å–∫–∏ –ø—Ä–æ –±—ñ–∑–Ω–µ—Å –ª–æ–≥—ñ–∫—É")

if db_ok:
    # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑ DI
    repo = PostgresCustomerRepository()
    service = CustomerService(repository=repo)

    summary = service.get_customer_summary(1)
    if summary:
        print(f"\nüìä Customer Summary:")
        for key, value in summary.items():
            print(f"   {key}: {value}")


# %% [markdown]
#  ### 6.4 Factory Pattern

# %%
class RepositoryFactory:
    """Factory –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è repositories"""

    @staticmethod
    def create_customer_repository(env: str = 'production') -> ICustomerRepository:
        """–°—Ç–≤–æ—Ä—é—î –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π repository –≤ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–¥ environment"""

        if env == 'production':
            return PostgresCustomerRepository()
        elif env == 'test':
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ mock –¥–ª—è —Ç–µ—Å—Ç—ñ–≤
            return InMemoryCustomerRepository()
        else:
            raise ValueError(f"Unknown environment: {env}")

class InMemoryCustomerRepository(ICustomerRepository):
    """In-memory implementation –¥–ª—è —Ç–µ—Å—Ç—ñ–≤"""

    def __init__(self):
        self.customers: Dict[int, Customer] = {
            1: Customer(1, "Test", "User", "test@example.com"),
            2: Customer(2, "Mock", "Customer", "mock@example.com"),
        }

    def get_by_id(self, id: int) -> Optional[Customer]:
        return self.customers.get(id)

    def get_all(self) -> List[Customer]:
        return list(self.customers.values())

    def get_by_email(self, email: str) -> Optional[Customer]:
        for customer in self.customers.values():
            if customer.email == email:
                return customer
        return None

    def get_by_country(self, country: str) -> List[Customer]:
        return [c for c in self.customers.values() if c.country == country]

    def create(self, entity: Customer) -> Customer:
        new_id = max(self.customers.keys()) + 1
        entity.customer_id = new_id
        self.customers[new_id] = entity
        return entity

    def update(self, entity: Customer) -> bool:
        if entity.customer_id in self.customers:
            self.customers[entity.customer_id] = entity
            return True
        return False

    def delete(self, id: int) -> bool:
        if id in self.customers:
            del self.customers[id]
            return True
        return False

print("=" * 80)
print("FACTORY PATTERN")
print("=" * 80)

# Production
prod_repo = RepositoryFactory.create_customer_repository('production')
print(f"‚úÖ Production repo: {type(prod_repo).__name__}")

# Test
test_repo = RepositoryFactory.create_customer_repository('test')
print(f"‚úÖ Test repo: {type(test_repo).__name__}")

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è test repository
test_service = CustomerService(repository=test_repo)
test_summary = test_service.get_customer_summary(1)
print(f"\nüìä Test Summary: {test_summary}")


# %% [markdown]
#  ### üìä –í–∏—Å–Ω–æ–≤–∫–∏ –∑ Architectural Patterns
# 
# 
# 
#  **Patterns —è–∫—ñ —Ä–æ–∑–≥–ª—è–Ω—É–ª–∏:**
# 
#  - ‚úÖ **Repository Pattern** - –∞–±—Å—Ç—Ä–∞–∫—Ü—ñ—è data access
# 
#  - ‚úÖ **Dependency Injection** - loose coupling
# 
#  - ‚úÖ **Factory Pattern** - flexible object creation
# 
#  - ‚úÖ **Service Layer** - business logic separation
# 
# 
# 
#  **–ü–µ—Ä–µ–≤–∞–≥–∏:**
# 
#  - ‚úÖ Testable code (–ª–µ–≥–∫–æ –ø–∏—Å–∞—Ç–∏ unit tests)
# 
#  - ‚úÖ Maintainable (–ª–µ–≥–∫–æ –º—ñ–Ω—è—Ç–∏ implementation)
# 
#  - ‚úÖ SOLID principles
# 
#  - ‚úÖ Clean Architecture
# 
# 
# 
#  **–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
# 
#  - ‚úÖ Production applications
# 
#  - ‚úÖ –°–µ—Ä–µ–¥–Ω—ñ —Ç–∞ –≤–µ–ª–∏–∫—ñ –ø—Ä–æ—î–∫—Ç–∏
# 
#  - ‚úÖ –ö–æ–º–∞–Ω–¥–Ω–∞ —Ä–æ–∑—Ä–æ–±–∫–∞
# 
#  - ‚ùå –ù–ï –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö —Å–∫—Ä–∏–ø—Ç—ñ–≤ (over-engineering)

# %% [markdown]
#  ---
# 
#  ## 7Ô∏è‚É£ ML Feature Store
# 
# 
# 
#  ### üéØ –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ:
# 
#  - –°—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ Feature Store –¥–ª—è ML
# 
#  - –í–∏–∫–æ–Ω—É–≤–∞—Ç–∏ feature engineering
# 
#  - –†–æ–∑–¥—ñ–ª—è—Ç–∏ offline/online stores
# 
#  - –ü—ñ–¥—Ç—Ä–∏–º—É–≤–∞—Ç–∏ point-in-time correctness

# %% [markdown]
#  ### 7.1 Feature Store Architecture
# 
# 
# 
#  **Feature Store** - —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–µ —Å—Ö–æ–≤–∏—â–µ –¥–ª—è ML features:
# 
#  - **Offline Store** - historical features –¥–ª—è training
# 
#  - **Online Store** - real-time features –¥–ª—è inference
# 
#  - **Feature Registry** - metadata –ø—Ä–æ features

# %%
class FeatureStore:
    """Simplified Feature Store implementation"""

    def __init__(self, db_connection_params: Dict[str, str]):
        self.db_params = db_connection_params

    def compute_customer_features(self) -> pd.DataFrame:
        """–û–±—á–∏—Å–ª–µ–Ω–Ω—è features –¥–ª—è –∫–ª—ñ—î–Ω—Ç—ñ–≤"""
        if not db_ok:
            print("‚ö†Ô∏è DB not available")
            return pd.DataFrame()

        print("=" * 80)
        print("FEATURE ENGINEERING")
        print("=" * 80)

        query = """
        WITH customer_orders AS (
            SELECT
                c.id AS customer_id,
                COUNT(DISTINCT o.id) AS total_orders,
                SUM(oi.quantity * oi.price_at_purchase) AS total_spent,
                MAX(o.order_date) AS last_order_date,
                MIN(o.order_date) AS first_order_date,
                COUNT(DISTINCT DATE_TRUNC('month', o.order_date)) AS active_months
            FROM customers c
            LEFT JOIN orders o       ON o.customer_id = c.id
            LEFT JOIN order_items oi ON oi.order_id   = o.id
            GROUP BY c.id
        )
        SELECT
            customer_id,
            COALESCE(total_orders, 0) AS total_orders,
            COALESCE(total_spent, 0) AS total_spent,
            -- —Å–µ—Ä–µ–¥–Ω—ñ–π —á–µ–∫: total_spent / –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–º–æ–≤–ª–µ–Ω—å
            COALESCE(total_spent, 0) / NULLIF(total_orders, 0) AS avg_order_value,
            EXTRACT(EPOCH FROM (CURRENT_DATE - last_order_date)) / 86400  AS recency_days,
            EXTRACT(EPOCH FROM (last_order_date - first_order_date)) / 86400 AS customer_lifetime_days,
            COALESCE(active_months, 0) AS active_months,
            -- RFM: —Å–≤—ñ–∂—ñ—à—ñ –¥–∞—Ç–∏ –º–∞—é—Ç—å –ë–Ü–õ–¨–®–ò–ô –±–∞–ª (5=–∫—Ä–∞—â–µ)
            (6 - NTILE(5) OVER (ORDER BY last_order_date DESC)) AS recency_score,
            NTILE(5) OVER (ORDER BY total_orders)               AS frequency_score,
            NTILE(5) OVER (ORDER BY total_spent)                AS monetary_score
        FROM customer_orders
        WHERE total_orders > 0;
        """

        conn = psycopg2.connect(**self.db_params)
        df = pd.read_sql_query(query, conn)
        conn.close()

        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ computed features
        df['avg_orders_per_month'] = df['total_orders'] / df['active_months'].replace(0, 1)
        df['rfm_score'] = df['recency_score'] + df['frequency_score'] + df['monetary_score']

        # CLV (–¥—É–∂–µ –ø—Ä–æ—Å—Ç–∞ –µ–≤—Ä–∏—Å—Ç–∏–∫–∞)
        df['predicted_clv'] = df['avg_order_value'] * df['avg_orders_per_month'] * 12

        # –ü—Ä–æ—Å—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞ —Ä–∏–∑–∏–∫—É –≤—ñ–¥—Ç–æ–∫—É
        df['churn_risk'] = (
            (df['recency_days'] > 90).astype(int) * 0.5 +
            (df['total_orders'] < 3).astype(int) * 0.3 +
            (df['avg_order_value'] < df['avg_order_value'].median()).astype(int) * 0.2
        )

        print(f"‚úÖ Computed features for {len(df)} customers")
        print(f"‚úÖ Total features: {len(df.columns)}")

        return df

    def save_features(self, entity_type: str, features_df: pd.DataFrame):
        """–ó–±–µ—Ä—ñ–≥–∞—î features –≤ offline store"""
        if not db_ok:
            print("‚ö†Ô∏è DB not available")
            return

        print("=" * 80)
        print("SAVING TO OFFLINE STORE")
        print("=" * 80)

        table_name = f"features_{entity_type}"

        with get_db_connection() as conn:
            cursor = conn.cursor()

            cursor.execute(f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    customer_id INTEGER PRIMARY KEY,
                    total_orders INTEGER,
                    total_spent DECIMAL(12,2),
                    avg_order_value DECIMAL(12,2),
                    recency_days DECIMAL(12,2),
                    customer_lifetime_days DECIMAL(12,2),
                    active_months INTEGER,
                    recency_score INTEGER,
                    frequency_score INTEGER,
                    monetary_score INTEGER,
                    avg_orders_per_month DECIMAL(12,2),
                    rfm_score INTEGER,
                    predicted_clv DECIMAL(12,2),
                    churn_risk DECIMAL(6,4),
                    computed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
            """)

            for _, row in features_df.iterrows():
                cursor.execute(f"""
                    INSERT INTO {table_name}
                    (customer_id, total_orders, total_spent, avg_order_value,
                     recency_days, customer_lifetime_days, active_months,
                     recency_score, frequency_score, monetary_score,
                     avg_orders_per_month, rfm_score, predicted_clv, churn_risk)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (customer_id) DO UPDATE SET
                        total_orders = EXCLUDED.total_orders,
                        total_spent = EXCLUDED.total_spent,
                        avg_order_value = EXCLUDED.avg_order_value,
                        recency_days = EXCLUDED.recency_days,
                        customer_lifetime_days = EXCLUDED.customer_lifetime_days,
                        active_months = EXCLUDED.active_months,
                        recency_score = EXCLUDED.recency_score,
                        frequency_score = EXCLUDED.frequency_score,
                        monetary_score = EXCLUDED.monetary_score,
                        avg_orders_per_month = EXCLUDED.avg_orders_per_month,
                        rfm_score = EXCLUDED.rfm_score,
                        predicted_clv = EXCLUDED.predicted_clv,
                        churn_risk = EXCLUDED.churn_risk,
                        computed_at = CURRENT_TIMESTAMP;
                """, (
                    int(row['customer_id']), int(row['total_orders']), float(row['total_spent']),
                    float(row['avg_order_value']), float(row['recency_days']),
                    float(row['customer_lifetime_days']), int(row['active_months']),
                    int(row['recency_score']), int(row['frequency_score']), int(row['monetary_score']),
                    float(row['avg_orders_per_month']), int(row['rfm_score']),
                    float(row['predicted_clv']), float(row['churn_risk'])
                ))

            cursor.close()

        print(f"‚úÖ Saved {len(features_df)} feature vectors to {table_name}")

    def get_features(self, entity_type: str, entity_ids: List[int]) -> pd.DataFrame:
        """–û—Ç—Ä–∏–º—É—î features –¥–ª—è inference (online store)"""
        if not db_ok:
            return pd.DataFrame()

        table_name = f"features_{entity_type}"
        conn = psycopg2.connect(**self.db_params)

        placeholders = ','.join(['%s'] * len(entity_ids))
        query = f"SELECT * FROM {table_name} WHERE customer_id IN ({placeholders});"

        df = pd.read_sql_query(query, conn, params=entity_ids)
        conn.close()
        return df


# %% [markdown]
#  ### 7.2 ML Model Integration

# %%
if db_ok and 'features_df' in locals():
    print("=" * 80)
    print("ML MODEL INTEGRATION")
    print("=" * 80)

    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è ML
    feature_columns = [
        'total_orders', 'total_spent', 'avg_order_value',
        'recency_days', 'customer_lifetime_days', 'active_months',
        'recency_score', 'frequency_score', 'monetary_score'
    ]

    X = features_df[feature_columns].fillna(0)
    y = (features_df['churn_risk'] > 0.5).astype(int)

    print(f"‚úÖ Features shape: {X.shape}")
    print(f"‚úÖ Target distribution:")
    print(f"   No churn: {(y==0).sum()}")
    print(f"   Churn: {(y==1).sum()}")

    print("\nüí° –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏:")
    print("   1. Train/test split")
    print("   2. Model training (XGBoost, LightGBM)")
    print("   3. Model evaluation")
    print("   4. Feature importance analysis")
    print("   5. Model deployment")
    print("   6. Online inference —á–µ—Ä–µ–∑ Feature Store")


# %% [markdown]
#  ### üìä –í–∏—Å–Ω–æ–≤–∫–∏ –∑ Feature Store
# 
# 
# 
#  **–©–æ –º–∏ –Ω–∞–≤—á–∏–ª–∏—Å—è:**
# 
#  - ‚úÖ –°—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–µ —Å—Ö–æ–≤–∏—â–µ –¥–ª—è features
# 
#  - ‚úÖ Feature engineering –∑ SQL —Ç–∞ pandas
# 
#  - ‚úÖ Offline store –¥–ª—è training
# 
#  - ‚úÖ Online store –¥–ª—è inference
# 
#  - ‚úÖ RFM, CLV, Churn risk computation
# 
# 
# 
#  **Production Best Practices:**
# 
#  - ‚úÖ **Point-in-time correctness** - features –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–æ–º–µ–Ω—Ç—É —á–∞—Å—É
# 
#  - ‚úÖ **Feature versioning** - track changes
# 
#  - ‚úÖ **Monitoring** - data drift detection
# 
#  - ‚úÖ **Documentation** - feature definitions
# 
#  - ‚úÖ **Access control** - —Ö—Ç–æ –º–æ–∂–µ —á–∏—Ç–∞—Ç–∏/–ø–∏—Å–∞—Ç–∏
# 
# 
# 
#  **Tools –¥–ª—è Production:**
# 
#  - Feast - open-source feature store
# 
#  - Tecton - managed feature platform
# 
#  - AWS SageMaker Feature Store
# 
#  - Databricks Feature Store

# %% [markdown]
#  ---
# 
#  ## 8Ô∏è‚É£ Advanced SQL Analytics
# 
# 
# 
#  ### üéØ –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ:
# 
#  - Cohort analysis –¥–ª—è retention
# 
#  - Funnel analysis –¥–ª—è conversion
# 
#  - Time-series analysis
# 
#  - Statistical functions

# %% [markdown]
#  ### 8.1 Cohort Analysis

# %%
if db_ok:
    print("=" * 80)
    print("COHORT ANALYSIS - Retention by Registration Month")
    print("=" * 80)

    cohort_query = """
    WITH base AS (
        SELECT
            c.id AS customer_id,
            DATE_TRUNC('month', c.registration_date)::date AS cohort_month,
            o.id AS order_id,
            o.order_date,
            DATE_TRUNC('month', o.order_date)::date AS order_month
        FROM customers c
        LEFT JOIN orders o ON o.customer_id = c.id
    ),
    cohort_sizes AS (
        SELECT
            cohort_month,
            COUNT(DISTINCT customer_id) AS cohort_size
        FROM base
        GROUP BY cohort_month
    ),
    cohort_activity AS (
        SELECT
            cohort_month,
            order_month,
            COUNT(DISTINCT customer_id) AS active_customers,
            (
              (EXTRACT(YEAR  FROM order_month) * 12 + EXTRACT(MONTH FROM order_month)) -
              (EXTRACT(YEAR  FROM cohort_month) * 12 + EXTRACT(MONTH FROM cohort_month))
            )::int AS months_since_registration
        FROM base
        WHERE order_month IS NOT NULL
        GROUP BY cohort_month, order_month
    )
    SELECT
        ca.cohort_month,
        cs.cohort_size,
        ca.months_since_registration,
        ca.active_customers,
        ROUND(100.0 * ca.active_customers / NULLIF(cs.cohort_size, 0), 2) AS retention_rate
    FROM cohort_activity ca
    JOIN cohort_sizes cs USING (cohort_month)
    WHERE ca.months_since_registration BETWEEN 0 AND 6
    ORDER BY ca.cohort_month, ca.months_since_registration
    LIMIT 20;
    """

    conn = psycopg2.connect(
        host="localhost",
        database="learning_db",
        user="admin",
        password="admin123"
    )
    cohort_df = pd.read_sql_query(cohort_query, conn)
    conn.close()

    print(f"\n{'Cohort':12} | {'Size':>6} | {'Month':>6} | {'Active':>8} | {'Retention':>10}")
    print("-" * 80)
    for _, row in cohort_df.head(15).iterrows():
        print(f"{str(row['cohort_month'])[:10]:12} | "
              f"{int(row['cohort_size']):6d} | "
              f"{int(row['months_since_registration']):6d} | "
              f"{int(row['active_customers']):8d} | "
              f"{row['retention_rate']:9.1f}%")


# %% [markdown]
#  ### 8.2 Funnel Analysis

# %%
if db_ok:
    print("=" * 80)
    print("FUNNEL ANALYSIS - Purchase Conversion")
    print("=" * 80)

    funnel_query = """
    WITH base AS (
        SELECT
            c.id AS customer_id,
            o.id AS order_id,
            oi.order_id AS has_item
        FROM customers c
        LEFT JOIN orders o       ON o.customer_id = c.id
        LEFT JOIN order_items oi ON oi.order_id   = o.id
    ),
    order_counts AS (
        SELECT customer_id, COUNT(*) AS orders
        FROM orders
        GROUP BY customer_id
    ),
    funnel AS (
        SELECT
            COUNT(DISTINCT b.customer_id)                                         AS registered_users,
            COUNT(DISTINCT CASE WHEN b.order_id  IS NOT NULL THEN b.customer_id END) AS users_with_orders,
            COUNT(DISTINCT CASE WHEN b.has_item IS NOT NULL THEN b.customer_id END)  AS users_with_items,
            COUNT(DISTINCT CASE WHEN oc.orders >= 2 THEN oc.customer_id END)         AS repeat_customers
        FROM base b
        LEFT JOIN order_counts oc ON oc.customer_id = b.customer_id
    )
    SELECT
        registered_users,
        users_with_orders,
        users_with_items,
        repeat_customers,
        ROUND(100.0 * users_with_orders / NULLIF(registered_users, 0), 2) AS conversion_to_order,
        ROUND(100.0 * users_with_items / NULLIF(users_with_orders, 0), 2) AS conversion_to_items,
        ROUND(100.0 * repeat_customers / NULLIF(users_with_orders, 0), 2) AS conversion_to_repeat
    FROM funnel;
    """


    conn = psycopg2.connect(
        host="localhost",
        database="learning_db",
        user="admin",
        password="admin123"
    )

    funnel_df = pd.read_sql_query(funnel_query, conn)
    conn.close()

    if len(funnel_df) > 0:
        row = funnel_df.iloc[0]
        print(f"\nüìä Conversion Funnel:")
        print(f"   1. Registered Users:    {row['registered_users']:6} (100.0%)")
        print(f"   2. Users with Orders:   {row['users_with_orders']:6} ({row['conversion_to_order']:5.1f}%)")
        print(f"   3. Users with Items:    {row['users_with_items']:6} ({row['conversion_to_items']:5.1f}%)")
        print(f"   4. Repeat Customers:    {row['repeat_customers']:6} ({row['conversion_to_repeat']:5.1f}%)")


# %% [markdown]
#  ### 8.3 Time-Series Analysis

# %%
timeseries_query_0 = """
WITH daily_sales AS (
    SELECT
        DATE_TRUNC('day', o.order_date)::date AS sale_date,
        SUM(oi.quantity * oi.price_at_purchase) AS daily_revenue,
        COUNT(DISTINCT o.id) AS daily_orders
    FROM orders o
    JOIN order_items oi ON oi.order_id = o.id
    GROUP BY DATE_TRUNC('day', o.order_date)
)
SELECT
    sale_date,
    daily_revenue,
    daily_orders,
    AVG(daily_revenue) OVER (
        ORDER BY sale_date
        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
    ) AS revenue_7day_ma,
    AVG(daily_revenue) OVER (
        ORDER BY sale_date
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
    ) AS revenue_30day_ma
FROM daily_sales
ORDER BY sale_date DESC
LIMIT 15;
"""


# %%
if db_ok:
    print("=" * 80)
    print("TIME-SERIES ANALYSIS - Moving Averages")
    print("=" * 80)

    timeseries_query = """
    WITH daily_sales AS (
        SELECT
            DATE_TRUNC('day', o.order_date)::date AS sale_date,
            SUM(oi.quantity * oi.price_at_purchase) AS daily_revenue,
            COUNT(DISTINCT o.id) AS daily_orders
        FROM orders o
        JOIN order_items oi ON oi.order_id = o.id
        GROUP BY DATE_TRUNC('day', o.order_date)
    ),
    series AS (
        SELECT generate_series(min(sale_date), max(sale_date), interval '1 day')::date AS sale_date
        FROM daily_sales
    ),
    filled AS (
        SELECT s.sale_date,
            COALESCE(ds.daily_revenue, 0) AS daily_revenue,
            COALESCE(ds.daily_orders, 0) AS daily_orders
        FROM series s
        LEFT JOIN daily_sales ds USING (sale_date)
    )
    SELECT
        sale_date,
        daily_revenue,
        daily_orders,
        AVG(daily_revenue) OVER (ORDER BY sale_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)  AS revenue_7day_ma,
        AVG(daily_revenue) OVER (ORDER BY sale_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS revenue_30day_ma
    FROM filled
    ORDER BY sale_date DESC
    LIMIT 15;
    """


    conn = psycopg2.connect(
        host="localhost",
        database="learning_db",
        user="admin",
        password="admin123"
    )

    ts_df = pd.read_sql_query(timeseries_query, conn)
    conn.close()

    print(f"\n{'Date':12} | {'Revenue':>10} | {'Orders':>7} | {'7-day MA':>10} | {'30-day MA':>10}")
    print("-" * 80)
    for _, row in ts_df.iterrows():
        print(f"{str(row['sale_date'])[:10]:12} | {row['daily_revenue']:10.2f} | {row['daily_orders']:7} | {row['revenue_7day_ma']:10.2f} | {row['revenue_30day_ma']:10.2f}")


# %% [markdown]
#  ### üìä –í–∏—Å–Ω–æ–≤–∫–∏ –∑ Advanced SQL
# 
# 
# 
#  **–©–æ –º–∏ –Ω–∞–≤—á–∏–ª–∏—Å—è:**
# 
#  - ‚úÖ **Cohort Analysis** - retention metrics
# 
#  - ‚úÖ **Funnel Analysis** - conversion optimization
# 
#  - ‚úÖ **Time-Series** - moving averages, trends
# 
#  - ‚úÖ **Window Functions** - LAG, LEAD, NTILE
# 
#  - ‚úÖ **CTEs** - —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–ø–∏—Ç–∏ –∑ –ø—ñ–¥–∑–∞–ø–∏—Ç–∞–º–∏
# 
# 
# 
#  **–Ü–Ω—à—ñ Advanced —Ç–µ—Ö–Ω—ñ–∫–∏ (–≤ sql_examples/05_advanced_analytics.sql):**
# 
#  - Market Basket Analysis (product affinity)
# 
#  - Statistical Functions (percentiles, correlation)
# 
#  - Outlier Detection
# 
#  - ML Feature Engineering SQL
# 
# 
# 
#  **Production Tips:**
# 
#  - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ materialized views –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∞–Ω–∞–ª—ñ—Ç–∏–∫
# 
#  - –Ü–Ω–¥–µ–∫—Å—É–π—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –¥–∞—Ç —Ç–∞ JOIN –∫–ª—é—á—ñ–≤
# 
#  - –ü–∞—Ä—Ç–∏—Ü—ñ–æ–Ω—É–π—Ç–µ –≤–µ–ª–∏–∫—ñ —Ç–∞–±–ª–∏—Ü—ñ –ø–æ –¥–∞—Ç–∞—Ö
# 
#  - –ö–µ—à—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ Redis –¥–ª—è real-time dashboards

# %% [markdown]
#  ---
# 
#  # üéâ –í—ñ—Ç–∞—î–º–æ! –í–∏ –∑–∞–≤–µ—Ä—à–∏–ª–∏ –ø–æ–≤–Ω–∏–π tutorial
# 
#  ---
# 
# 
# 
#  ## üìö –©–æ –≤–∏ –≤–∏–≤—á–∏–ª–∏:
# 
# 
# 
#  ### –ë–∞–∑–æ–≤—ñ –ú–æ–¥—É–ª—ñ:
# 
#  - ‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –∑ asyncio —Ç–∞ aiohttp
# 
#  - ‚úÖ SQL –æ—Å–Ω–æ–≤–∏: SELECT, JOIN, GROUP BY, Window Functions
# 
#  - ‚úÖ Python + PostgreSQL –∑ psycopg2
# 
#  - ‚úÖ –ê–Ω–∞–ª—ñ–∑ –¥–∞–Ω–∏—Ö –∑ pandas —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
# 
# 
# 
#  ### Advanced –ú–æ–¥—É–ª—ñ:
# 
#  - ‚úÖ Production ETL Pipeline –∑ metrics —Ç–∞ validation
# 
#  - ‚úÖ Architectural Patterns: Repository, DI, Factory
# 
#  - ‚úÖ ML Feature Store –¥–ª—è feature engineering
# 
#  - ‚úÖ Advanced SQL Analytics: cohort, funnel, time-series
# 
# 
# 
#  ## üöÄ –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏:
# 
# 
# 
#  1. **–ü—Ä–∞–∫—Ç–∏–∫–∞** - –ú–æ–¥–∏—Ñ—ñ–∫—É–π—Ç–µ –∫–æ–¥ –¥–ª—è —Å–≤–æ—ó—Ö use cases
# 
#  2. **–†–æ–∑—à–∏—Ä–µ–Ω–Ω—è** - –î–æ–¥–∞–π—Ç–µ –≤–ª–∞—Å–Ω—ñ features
# 
#  3. **Production** - –î–æ–¥–∞–π—Ç–µ —Ç–µ—Å—Ç–∏, –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥, CI/CD
# 
#  4. **–ü–æ–¥—ñ–ª—ñ—Ç—å—Å—è** - –î–æ–¥–∞–π—Ç–µ –ø—Ä–æ—î–∫—Ç –≤ portfolio
# 
# 
# 
#  ## üìñ –ö–æ—Ä–∏—Å–Ω—ñ –†–µ—Å—É—Ä—Å–∏:
# 
# 
# 
#  - [Python asyncio docs](https://docs.python.org/3/library/asyncio.html)
# 
#  - [PostgreSQL docs](https://www.postgresql.org/docs/)
# 
#  - [pandas docs](https://pandas.pydata.org/docs/)
# 
#  - [Feast Feature Store](https://feast.dev/)
# 
# 
# 
#  ## üí¨ Feedback:
# 
# 
# 
#  –í—ñ–¥–∫—Ä–∏–≤–∞–π—Ç–µ Issues –∞–±–æ Pull Requests –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º–∞—Ç–µ—Ä—ñ–∞–ª—É!
# 
# 
# 
#  ---
# 
#  **Happy Coding! üéâ**


