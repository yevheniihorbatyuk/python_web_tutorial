# Module 8: Quick Reference Guide

## File Structure

```
module_08/
â”œâ”€â”€ 01_sqlalchemy_advanced.py      # ORM with User-Address-City-Country
â”œâ”€â”€ 02_mongodb_advanced.py          # Document storage with aggregation
â”œâ”€â”€ 03_caching_strategies.py        # LRU cache + Redis caching
â”œâ”€â”€ 04_rabbitmq_messaging.py        # Producer/consumer async processing
â”œâ”€â”€ 05_realworld_data_science.py    # Integrated data science system
â”œâ”€â”€ COMPREHENSIVE_GUIDE.md          # Full documentation
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md       # What was built
â”œâ”€â”€ QUICK_REFERENCE.md             # This file
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ docker-compose.yml             # Service containers
â”œâ”€â”€ .env.example                   # Environment configuration
â””â”€â”€ README.md                      # Getting started
```

---

## 1ï¸âƒ£ SQLAlchemy Advanced (01_sqlalchemy_advanced.py)

### Key Classes:
```python
User â†’ Address â†’ City â†’ Country (hierarchy)

UserRepository:
  .find_users_by_country("Ukraine")
  .find_users_by_city("Kyiv")
  .get_users_with_addresses()
  .get_user_statistics()
  .get_users_ranked_by_activity()

CountryRepository:
  .get_all_countries_with_cities()
  .get_country_statistics()

DataScienceAnalytics:
  .get_user_distribution_by_country()
  .get_user_metrics_by_registration_cohort()
  .identify_high_value_users()
```

### Core Patterns:
- **N+1 Prevention**: `joinedload(User.address).joinedload(Address.city)`
- **Aggregations**: `func.count()`, `func.avg()`, `func.max()`
- **Transactions**: `session.begin_nested()` with rollback
- **Query Building**: Filter, order_by, limit, offset

### Real-World Use Cases:
- Geographic user analysis
- Cohort-based retention analysis
- High-value user identification
- Regional expansion planning

---

## 2ï¸âƒ£ MongoDB Advanced (02_mongodb_advanced.py)

### Data Models:
```python
Event Document:
{
  "user_id": 123,
  "event_type": "user_purchase",
  "timestamp": ISODate(),
  "metadata": { "price": 99.99, "category": "electronics" },
  "geo": { "country": "Ukraine", "coordinates": [lon, lat] }
}

User Profile:
{
  "username": "data_scientist",
  "email": "user@example.com",
  "profile": { "interests": [...], "expertise_level": "senior" },
  "metrics": { "engagement_score": 8.5 },
  "subscriptions": [...]
}
```

### Key Methods:
```python
UserManager:
  .insert_user(user_data)
  .find_users_by_interest("data-science")
  .bulk_insert_users(users_list)
  .update_user_metrics(user_id, metrics)

EventTracker:
  .log_event(event_data)
  .get_event_distribution()          # $group aggregation
  .get_user_activity_metrics()       # Complex pipeline
  .get_purchase_analytics()          # Business intelligence

GeoLocationManager:
  .find_events_near_location(lon, lat, max_distance)

DataProcessor:
  .calculate_cohort_metrics()
  .export_data_for_analysis()
```

### Aggregation Pipelines:
```python
# Event distribution
[
  { "$group": { "_id": "$event_type", "count": { "$sum": 1 } } },
  { "$sort": { "count": -1 } }
]

# User activity metrics
[
  { "$match": { "timestamp": { "$gte": start, "$lte": end } } },
  { "$group": { "_id": "$user_id", "event_count": { "$sum": 1 } } },
  { "$sort": { "event_count": -1 } },
  { "$limit": 10 }
]
```

### Indexes:
```python
# Unique indexes
collection.create_index([("username", 1)], unique=True)

# Compound indexes
collection.create_index([("user_id", 1), ("timestamp", -1)])

# Geospatial indexes
collection.create_index([("geo.coordinates", "2dsphere")])
```

### Real-World Use Cases:
- Event tracking systems
- IoT sensor data
- User behavior analytics
- Time-series storage
- User-generated content

---

## 3ï¸âƒ£ Caching Strategies (03_caching_strategies.py)

### Tier 1: In-Process (@lru_cache)
```python
@lru_cache(maxsize=128)
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Results:
# fib(30): 179ms â†’ 0.02ms (11,000x faster!)
```

### Tier 2: Distributed (Redis)
```python
@redis_cache(ttl=3600)
def get_user_recommendations(user_id):
    # 1 hour TTL
    return compute_recommendations(user_id)
```

### Tier 3: Application Logic (Custom TTL)
```python
@timed_lru_cache(maxsize=1000, ttl_seconds=600)
def get_trending_items(category):
    # 10 minute TTL with auto-expiration
    return compute_trending(category)
```

### Manual Redis Cache:
```python
redis_cache = RedisCache(redis_client)

# Get cached value
value = redis_cache.get("user:123:profile")

# Set with TTL
redis_cache.set("user:123:profile", user_profile, ttl=3600)

# Delete
redis_cache.delete("user:123:profile")

# Clear by pattern
redis_cache.clear_pattern("user:123:*")
```

### Performance Benchmarks:
| Function | Without Cache | With @lru_cache | Speedup |
|----------|---------------|-----------------|---------|
| Fibonacci(25) | 16.44ms | 0.01ms | 1,271x |
| Fibonacci(30) | 179.19ms | 0.02ms | 11,456x |
| Fibonacci(35) | Timeout | 0.01ms | âˆ |

### Best Practice Rules:
```
Use @lru_cache if:
  âœ“ Pure function (no side effects)
  âœ“ Deterministic (same input = same output)
  âœ“ Small result size
  âœ— Shared across processes

Use Redis if:
  âœ“ Need distributed caching
  âœ“ Large caches (1000s of items)
  âœ“ Multi-process/server environment
  âœ— Tiny datasets
  âœ— No network overhead tolerance

Use application logic if:
  âœ“ Custom eviction policy needed
  âœ“ Complex invalidation logic
  âœ“ Domain-specific caching
```

---

## 4ï¸âƒ£ RabbitMQ Messaging (04_rabbitmq_messaging.py)

### Message Types:
```python
UserDataMessage:
  {"username": "...", "email": "..."}

NotificationMessage:
  {"recipient": "...", "subject": "...", "content": "..."}

AnalyticsMessage:
  {"event_type": "...", "user_id": "...", "properties": {...}}
```

### Producer:
```python
producer = Producer()

# Publish user data
producer.publish_user_data_event("john_doe", "john@example.com")

# Publish notification
producer.publish_notification("user@example.com", "Welcome!", "Hello!")

# Publish analytics
producer.publish_analytics_event("user_signup", user_id=123, extra={...})
```

### Consumer Pattern:
```python
class MyConsumer(Consumer):
    def message_callback(self, message):
        # Process message
        # Return True if successful
        # Return False if should retry
        pass

consumer = MyConsumer()
consumer.start_consuming()
```

### Exchange Types:
```
Direct Exchange (data_processing):
  Producer â†’ routing_key="user.data" â†’ Queue â†’ Consumer

Topic Exchange (notifications):
  Producer â†’ routing_key="notification.email" â†’ Queue â†’ Consumer

Fanout Exchange (analytics):
  Producer â†’ All queues broadcast
```

### Error Handling:
```
Message Processing:
  1st attempt: Immediate
  2nd attempt: Wait 2^1 = 2 seconds
  3rd attempt: Wait 2^2 = 4 seconds
  Failed: Send to Dead Letter Queue (DLQ)
```

### Real-World Scenarios:
```
User Signup:
  Producer (API) â†’ publish_user_data_event()
     â†’ UserDataConsumer (validation, enrichment)
     â†’ MongoDB (store events)
     â†’ RabbitMQ (queue recommendations)
     â†’ RecommendationConsumer (calculate)
     â†’ Redis (cache results)
     â†’ API (return to user)
```

---

## 5ï¸âƒ£ Real-World Data Science (05_realworld_data_science.py)

### User Segments:
```python
DORMANT   = no activity for 90+ days â†’ Re-engagement campaigns
ACTIVE    = regular users â†’ Personalized recommendations
VIP       = high spending â†’ Premium features/support
AT_RISK   = churn signals â†’ Retention offers
```

### Prediction Types:
```python
CHURN = probability user will leave (0-100%)
LIFETIME_VALUE = total revenue expected ($)
NEXT_PURCHASE = estimated days until next buy
RECOMMENDATION = products user might like
```

### AnalyticsEngine:
```python
engine = AnalyticsEngine()
profile = engine.calculate_user_profile(user_data)

# Returns:
# {
#   "segment": "active",
#   "engagement": 65.5,
#   "churn_risk": 0.21,
#   "ltv_prediction": 1500.00
# }
```

### MLModelManager:
```python
ml = MLModelManager()

churn = ml.predict("churn", user_id=123)
# Returns: { "value": 0.35, "confidence": 0.85 }

ltv = ml.predict("lifetime_value", user_id=123)
# Returns: { "value": 5000.00, "confidence": 0.80 }
```

### RecommendationEngine:
```python
engine = RecommendationEngine()
recs = engine.get_recommendations(user_id=123)

# Returns: [
#   { "product_id": "P1", "relevance": 0.90 },
#   { "product_id": "P2", "relevance": 0.85 },
#   { "product_id": "P3", "relevance": 0.80 }
# ]
```

### InsightsGenerator:
```python
insights = InsightsGenerator()
report = insights.generate_segment_report(profiles)

# Segment-specific insights:
# - Dormant: "X users inactive, recommend re-engagement"
# - Active: "Y users active, personalize recommendations"
# - VIP: "Z premium users, offer exclusive perks"
# - At-risk: "W users at risk, send retention offers"
```

---

## ğŸ”„ Integration Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client/User    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     API Layer (Sync)                â”‚
    â”‚     SQLAlchemy ORM                  â”‚
    â”‚  (PostgreSQL, Redis Cache)          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                 â”‚
    â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Redis     â”‚            â”‚  RabbitMQ        â”‚
â”‚   Cache     â”‚            â”‚  Message Broker  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚                â”‚                â”‚
    â–¼              â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL â”‚ â”‚  MongoDB    â”‚ â”‚ Recommendationâ”‚ â”‚Analyticsâ”‚
â”‚ (ORM)      â”‚ â”‚  (Events)   â”‚ â”‚ Engine       â”‚ â”‚ Consumer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚              â”‚                â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Data Science Engine            â”‚
    â”‚  - User Segmentation            â”‚
    â”‚  - Churn Prediction             â”‚
    â”‚  - LTV Modeling                 â”‚
    â”‚  - Personalization              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Insights & Recommendations     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
        User Gets Results
```

---

## ğŸƒ Quick Start Commands

```bash
# Install dependencies
pip install --break-system-packages -r requirements.txt

# Run individual modules
python3 01_sqlalchemy_advanced.py
python3 02_mongodb_advanced.py
python3 03_caching_strategies.py
python3 04_rabbitmq_messaging.py
python3 05_realworld_data_science.py

# Set up full environment
docker-compose up -d

# Access services
# PostgreSQL: localhost:5432
# MongoDB: localhost:27017
# Redis: localhost:6379
# RabbitMQ Web UI: http://localhost:15672 (guest/guest)
```

---

## ğŸ“Š Performance Guidelines

| Operation | Time | Notes |
|-----------|------|-------|
| SQLAlchemy Query (no cache) | 50-100ms | Database I/O |
| SQLAlchemy Query (with eager load) | 50-100ms | Single query, prevent N+1 |
| Redis Query (hit) | 1-5ms | Network + deserialize |
| @lru_cache (hit) | <0.1ms | In-memory, fastest |
| MongoDB aggregation | 100-500ms | Depends on data volume |
| RabbitMQ message (publish) | 5-10ms | Fast queue operation |
| ML Prediction (first) | 200-500ms | Model computation |
| ML Prediction (cached) | <1ms | Redis cache |

---

## ğŸ” Security Checklist

- âœ… Use parameterized queries (SQLAlchemy automatic)
- âœ… Validate input before processing
- âœ… Use environment variables for secrets
- âœ… Enable SSL/TLS for all network connections
- âœ… Limit database user permissions
- âœ… Use message encryption for sensitive data
- âœ… Implement rate limiting on APIs
- âœ… Log security events

---

## ğŸ“š When to Use Each Technology

### PostgreSQL + SQLAlchemy
```
Use when:
âœ“ Structured data with clear schema
âœ“ ACID transactions required
âœ“ Complex joins and queries
âœ“ Referential integrity critical

Examples:
- User accounts
- E-commerce orders
- Financial transactions
- Business records
```

### MongoDB
```
Use when:
âœ“ Schema evolves frequently
âœ“ Large volumes of unstructured data
âœ“ Document-level transactions OK
âœ“ Horizontal scaling needed

Examples:
- Event logs
- IoT sensor data
- User-generated content
- Time-series data
```

### Redis
```
Use when:
âœ“ Sub-millisecond latency needed
âœ“ Data fits in memory
âœ“ Caching layer wanted
âœ“ Session storage

Examples:
- Web caches
- Session storage
- Real-time leaderboards
- Rate limiting counters
```

### RabbitMQ
```
Use when:
âœ“ Need async processing
âœ“ Services must be decoupled
âœ“ Reliability and retries important
âœ“ Scaling consumers independently

Examples:
- Email sending
- Image processing
- Data aggregation
- Event streaming
```

---

## ğŸ¯ Common Patterns

### Pattern 1: Cache-Aside
```python
def get_user(user_id):
    # Check cache first
    user = redis.get(f"user:{user_id}")
    if user:
        return user

    # Cache miss, fetch from DB
    user = db.get_user(user_id)

    # Store in cache
    redis.set(f"user:{user_id}", user, ttl=3600)

    return user
```

### Pattern 2: Write-Through Cache
```python
def update_user(user_id, data):
    # Update DB
    user = db.update_user(user_id, data)

    # Update cache
    redis.set(f"user:{user_id}", user, ttl=3600)

    return user
```

### Pattern 3: Read-Heavy Aggregation
```python
# Store pre-computed results
daily_stats = calculate_stats()  # Expensive operation
redis.set("stats:daily", daily_stats, ttl=86400)  # 24 hours

# Serve from cache
stats = redis.get("stats:daily")
```

### Pattern 4: Message Queue Processing
```
User action â†’ Producer â†’ RabbitMQ Queue â†’ Consumer â†’ Update DB
  (fast)       (instant)      (async)    (background)
```

---

## âš ï¸ Common Pitfalls

1. **N+1 Queries**: Always use `joinedload()` for relationships
2. **Cache Invalidation**: Remember to clear cache on updates
3. **Connection Pools**: Configure appropriate pool sizes
4. **Message Loss**: Implement acknowledgment and retries
5. **Unbounded Cache**: Set maximum size and TTL
6. **No Monitoring**: Track cache hits/misses, query times
7. **Blocking Calls**: Use async for I/O-heavy operations

---

**Happy Coding!** ğŸš€
